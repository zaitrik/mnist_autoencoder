{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This is how a neural network learns to add, multiply and compare handwritten digits Without knowing their values "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\"> <img src=\"https://i.dlpng.com/static/png/6906777_preview.png\"> </p>   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I described in a [previous post](https://blog.jovian.ai/how-to-train-supervised-machine-learning-algorithms-without-labeled-data-6ebddc01a00f), how useful are autoencoders in  automated labeling. The main property of these networks is their ability to learn features/patterns in the data. This is in fact not specific to autoencoders and can be implemented using other unsupervised techniques, mainly **PCA**.  \r\n",
    "The ability to detect and learn features in data can be used in other areas.  \r\n",
    "\r\n",
    "In this post, I will present some applications of convolutional autoencoders:  \r\n",
    "- First, a convolutional autoencoder will be trained on **MNIST** data.\r\n",
    "- After the training of the encoder and decoder, we will freeze their weights and use them with additional dense layers to \"learn\" arithmetic operations, namely addition, multiplication and comparison.  \r\n",
    "The trick is to *never* explicitly associate the handwritten digits in **MNIST** dataset with their respective labels. We will see that the neural networks will be nevertheless able to reach 97+% accuracy in all cases on unseen data.\r\n",
    "\r\n",
    "The first step is described in the following diagram:\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/chLUEdp.png\"> </p>   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the second step, we will use the encoder in series with dense layers to perform arithmetic operations: addition, multiplication and comparison. We will train only the dense layer weights, and supply the results of the operations as labels. note that we will not supply the digits values (labels).\r\n",
    "\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/s8U8up4.png\"> </p> \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training an autoencoder on MNIST data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar to the previous article, we will use MNIST data in this experiment. The autoencoder will learn the handwritten digits features using 60000 training samples. We import MNIST using *KERAS* library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "#import libraries and setup \r\n",
    "import keras\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import logging\r\n",
    "logging.getLogger('tensorflow').disabled = True\r\n",
    "from keras.models import Sequential, Model\r\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, UpSampling2D, Reshape, Concatenate, Input\r\n",
    "from keras.callbacks import EarlyStopping\r\n",
    "from keras.utils.vis_utils import plot_model\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, restore_best_weights=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# import mnist\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n",
    "print(x_train.shape,y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We scale the data in the range `[0,1]` and reshape it to *KERAS* format for pictures (nbr_samples x width x height x channels) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#normalize data\r\n",
    "if x_train.max() >1:\r\n",
    "    x_train = x_train / 255\r\n",
    "    x_test = x_test / 255\r\n",
    "\r\n",
    "default_shape = x_train.shape\r\n",
    "#reshape input data to 1 channel\r\n",
    "x_train = x_train.reshape(-1,default_shape[1],default_shape[2],1)\r\n",
    "x_test = x_test.reshape(-1,default_shape[1],default_shape[2],1)\r\n",
    "image_dim = x_train.shape[1:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will implement a similar autoencoder architecture as in [[1]](https://blog.jovian.ai/how-to-train-supervised-machine-learning-algorithms-without-labeled-data-6ebddc01a00f). It is based on a series of convolutional layers, that will gradually encode the 28x28 image (784 pixel) into a 100 elements array, and decode that representation back to the original format. The resulting image -after the training step- will hopefully resemble to the original one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# create an autoencoder / decoder \r\n",
    "encoder = Sequential()\r\n",
    "encoder.add(Conv2D(32,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu',input_shape=image_dim))\r\n",
    "encoder.add(MaxPooling2D(2,2))\r\n",
    "encoder.add(Conv2D(64,kernel_size=(3,3), strides=(1,1),padding='same',activation='selu'))\r\n",
    "encoder.add(MaxPooling2D(2,2))\r\n",
    "encoder.add(Conv2D(128,kernel_size=(3,3), strides=(1,1),padding='same',activation='selu'))\r\n",
    "encoder.add(Flatten())\r\n",
    "encoder.add(Dense(100,activation='sigmoid'))\r\n",
    "encoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               627300    \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 719,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "encoder_out_dim = encoder.layers[-1].output_shape[1:] # dimension of the encoder output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "decoder = Sequential()\r\n",
    "decoder.add(Dense(6272, activation='sigmoid', input_shape=encoder_out_dim))\r\n",
    "decoder.add(Reshape(( 7, 7, 128)))\r\n",
    "decoder.add(Conv2D(128,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu'))\r\n",
    "decoder.add(UpSampling2D((2,2)))\r\n",
    "decoder.add(Conv2D(64,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu'))\r\n",
    "decoder.add(UpSampling2D((2,2)))\r\n",
    "decoder.add(Conv2D(1,kernel_size=(3,3), strides=(1,1),padding='same', activation='sigmoid'))\r\n",
    "\r\n",
    "decoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "=================================================================\n",
      "Total params: 855,425\n",
      "Trainable params: 855,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The autoencoder is created using the encoder and the decoder:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "enc_dec = Sequential([encoder,decoder])\r\n",
    "enc_dec.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_3 (Sequential)    (None, 100)               719972    \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 28, 28, 1)         855425    \n",
      "=================================================================\n",
      "Total params: 1,575,397\n",
      "Trainable params: 1,575,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It will be trained as a set of binary classifiers for each pixel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "enc_dec.compile(optimizer='nadam', loss = 'binary_crossentropy')\r\n",
    "history = enc_dec.fit(x_train,x_train, batch_size=1000,epochs=1000,validation_split=0.2, verbose=2,callbacks=[es,es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "48/48 - 10s - loss: 0.3642 - val_loss: 0.2719\n",
      "Epoch 2/1000\n",
      "48/48 - 10s - loss: 0.2715 - val_loss: 0.2730\n",
      "Epoch 3/1000\n",
      "48/48 - 10s - loss: 0.2666 - val_loss: 0.2650\n",
      "Epoch 4/1000\n",
      "48/48 - 10s - loss: 0.2644 - val_loss: 0.2637\n",
      "Epoch 5/1000\n",
      "48/48 - 10s - loss: 0.2393 - val_loss: 0.2135\n",
      "Epoch 6/1000\n",
      "48/48 - 10s - loss: 0.1882 - val_loss: 0.1649\n",
      "Epoch 7/1000\n",
      "48/48 - 10s - loss: 0.1552 - val_loss: 0.1430\n",
      "Epoch 8/1000\n",
      "48/48 - 10s - loss: 0.1366 - val_loss: 0.1323\n",
      "Epoch 9/1000\n",
      "48/48 - 10s - loss: 0.1287 - val_loss: 0.1562\n",
      "Epoch 10/1000\n",
      "48/48 - 10s - loss: 0.4014 - val_loss: 0.1864\n",
      "Epoch 11/1000\n",
      "48/48 - 10s - loss: 0.1611 - val_loss: 0.1513\n",
      "Epoch 12/1000\n",
      "48/48 - 10s - loss: 0.1406 - val_loss: 0.1345\n",
      "Epoch 13/1000\n",
      "48/48 - 10s - loss: 0.1305 - val_loss: 0.1288\n",
      "Epoch 14/1000\n",
      "48/48 - 10s - loss: 0.1241 - val_loss: 0.1244\n",
      "Epoch 15/1000\n",
      "48/48 - 10s - loss: 0.1191 - val_loss: 0.1170\n",
      "Epoch 16/1000\n",
      "48/48 - 10s - loss: 0.1144 - val_loss: 0.1148\n",
      "Epoch 17/1000\n",
      "48/48 - 10s - loss: 0.1115 - val_loss: 0.1119\n",
      "Epoch 18/1000\n",
      "48/48 - 10s - loss: 0.1080 - val_loss: 0.1101\n",
      "Epoch 19/1000\n",
      "48/48 - 10s - loss: 0.1057 - val_loss: 0.1017\n",
      "Epoch 20/1000\n",
      "48/48 - 10s - loss: 0.1032 - val_loss: 0.1022\n",
      "Epoch 21/1000\n",
      "48/48 - 10s - loss: 0.1015 - val_loss: 0.0989\n",
      "Epoch 22/1000\n",
      "48/48 - 10s - loss: 0.0985 - val_loss: 0.1025\n",
      "Epoch 23/1000\n",
      "48/48 - 10s - loss: 0.0978 - val_loss: 0.0964\n",
      "Epoch 24/1000\n",
      "48/48 - 10s - loss: 0.0959 - val_loss: 0.0985\n",
      "Epoch 25/1000\n",
      "48/48 - 10s - loss: 0.0953 - val_loss: 0.0922\n",
      "Epoch 26/1000\n",
      "48/48 - 10s - loss: 0.0933 - val_loss: 0.0932\n",
      "Epoch 27/1000\n",
      "48/48 - 10s - loss: 0.0923 - val_loss: 0.0908\n",
      "Epoch 28/1000\n",
      "48/48 - 10s - loss: 0.0911 - val_loss: 0.0962\n",
      "Epoch 29/1000\n",
      "48/48 - 10s - loss: 0.0905 - val_loss: 0.0920\n",
      "Epoch 30/1000\n",
      "48/48 - 10s - loss: 0.0894 - val_loss: 0.0876\n",
      "Epoch 31/1000\n",
      "48/48 - 10s - loss: 0.0890 - val_loss: 0.0875\n",
      "Epoch 32/1000\n",
      "48/48 - 10s - loss: 0.0874 - val_loss: 0.0864\n",
      "Epoch 33/1000\n",
      "48/48 - 10s - loss: 0.0877 - val_loss: 0.0858\n",
      "Epoch 34/1000\n",
      "48/48 - 10s - loss: 0.0860 - val_loss: 0.0877\n",
      "Epoch 35/1000\n",
      "48/48 - 10s - loss: 0.0861 - val_loss: 0.0889\n",
      "Epoch 36/1000\n",
      "48/48 - 10s - loss: 0.0853 - val_loss: 0.0860\n",
      "Epoch 37/1000\n",
      "48/48 - 10s - loss: 0.0842 - val_loss: 0.0842\n",
      "Epoch 38/1000\n",
      "48/48 - 10s - loss: 0.0848 - val_loss: 0.0864\n",
      "Epoch 39/1000\n",
      "48/48 - 10s - loss: 0.0835 - val_loss: 0.0853\n",
      "Epoch 40/1000\n",
      "48/48 - 10s - loss: 0.0835 - val_loss: 0.0821\n",
      "Epoch 41/1000\n",
      "48/48 - 10s - loss: 0.0829 - val_loss: 0.0830\n",
      "Epoch 42/1000\n",
      "48/48 - 10s - loss: 0.0824 - val_loss: 0.0834\n",
      "Epoch 43/1000\n",
      "48/48 - 10s - loss: 0.0819 - val_loss: 0.0829\n",
      "Epoch 44/1000\n",
      "48/48 - 10s - loss: 0.0815 - val_loss: 0.0815\n",
      "Epoch 45/1000\n",
      "48/48 - 10s - loss: 0.0812 - val_loss: 0.0830\n",
      "Epoch 46/1000\n",
      "48/48 - 10s - loss: 0.1027 - val_loss: 0.3337\n",
      "Epoch 47/1000\n",
      "48/48 - 10s - loss: 0.1360 - val_loss: 0.1045\n",
      "Epoch 48/1000\n",
      "48/48 - 10s - loss: 0.1002 - val_loss: 0.0904\n",
      "Epoch 49/1000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "48/48 - 10s - loss: 0.0933 - val_loss: 0.0945\n",
      "Epoch 00049: early stopping\n",
      "Epoch 00049: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a02de33b48>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The early stopping will make sure the autoencoder will not overfit the training data. There are two ways to verify the network. First, we can evaluate the loss function on test data, and expect it to be close to the loss value on the training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "enc_dec.evaluate(x_test,x_test,batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10/10 [==============================] - 1s 75ms/step - loss: 0.0801\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.08007277548313141"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "enc_dec.evaluate(x_train,x_train,batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60/60 [==============================] - 5s 75ms/step - loss: 0.0806\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.08060310781002045"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is very close, around `0.08` for both data sets. The second method is to check the resulting reconstitution that we obtain for a random sample from the test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "random_label = np.random.randint(0,9999)\r\n",
    "img_sample = x_test[random_label,:,:].reshape((1,28,28,1))\r\n",
    "plt.imshow(img_sample.reshape(28,28), cmap='gray');\r\n",
    "pred_img = enc_dec.predict(img_sample) \r\n",
    "plt.figure();\r\n",
    "plt.imshow(pred_img.reshape(28,28), cmap='gray');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*A picture is worth a thousand words!* Just to be on the safe side, I ran this test multiple times and the results were consistent. Let's save the encoder and the decoder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# save models\r\n",
    "encoder.save('encoder')\r\n",
    "decoder.save('decoder')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a trained encoder and decoder, let's focus on the *encoder*. For each image, is associated a representation that captures most of the interesting features. This representation is sufficient to reconstitute the image using the decoder. Here is the representation of the sample image we used earlier: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "representation_sample = encoder.predict(img_sample)\r\n",
    "print(representation_sample) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.18486543 0.5697806  0.25546345 0.513535   0.47046018 0.23013075\n",
      "  0.11007436 0.08223282 0.92961174 0.34727788 0.20599699 0.31574652\n",
      "  0.5037295  0.67289937 0.4258053  0.05625024 0.6901649  0.32541478\n",
      "  0.72563386 0.1158777  0.22629336 0.10287777 0.38414446 0.880189\n",
      "  0.18952799 0.22327262 0.49226147 0.04139815 0.6070379  0.67899853\n",
      "  0.1930634  0.9533057  0.26226565 0.3571257  0.10029613 0.64641505\n",
      "  0.7480828  0.6451894  0.19870374 0.8850102  0.65947354 0.6287513\n",
      "  0.6176214  0.5163804  0.25737226 0.14717074 0.09626418 0.78674656\n",
      "  0.8884231  0.7183272  0.40912864 0.25173852 0.6504716  0.73822445\n",
      "  0.21953063 0.794006   0.36661652 0.29903716 0.9652254  0.56687284\n",
      "  0.819398   0.10387004 0.18816645 0.631493   0.17954373 0.8222881\n",
      "  0.37873647 0.8320993  0.1338265  0.6080741  0.35885492 0.6325537\n",
      "  0.6959548  0.06192173 0.29763156 0.75721514 0.665339   0.24818887\n",
      "  0.19820885 0.23145263 0.32199287 0.87531734 0.49698454 0.5838976\n",
      "  0.2733242  0.7204009  0.9374683  0.5761113  0.94386744 0.5706298\n",
      "  0.14885998 0.9650065  0.30205414 0.67331403 0.43952814 0.7002536\n",
      "  0.5170414  0.36596012 0.20525713 0.72305924]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using these 100 numbers, we generate a 28x28 image (784 pixels)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "recons_image = decoder.predict(representation_sample)\r\n",
    "plt.imshow(recons_image.reshape(28,28), cmap='gray');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And here is where the *fun part* begins! using the lower-dimension representation, let's do some math."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning how to add two handwritten digits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The idea is simple. Using the representation of two images, we train a neural network to compute their sum. We will not provide the value of each digit, but we will provide the sum during the training step.  \r\n",
    "We will be performing addition between numbers in the range [0-9]. The results will be in the range [0-18]. So the results will be coded using two outputs:  \r\n",
    "1- Units, multiclass output [0,1,2,3,4,5,6,7,8,9]  \r\n",
    "2- Tens, binary output [0,1]  \r\n",
    "\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/Zgnd82F.png\"> </p> \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the functional API in *KERAS* we define the network architecture. First, we import the encoder *twice* and freeze its weights:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# duplicate encoders and freeze weights\r\n",
    "encoder1 = keras.models.load_model('encoder') \r\n",
    "encoder1._name = 'encoder1'\r\n",
    "encoder1.trainable = False\r\n",
    "\r\n",
    "encoder2 = keras.models.load_model('encoder')\r\n",
    "encoder2._name = 'encoder2'\r\n",
    "encoder2.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the encoders, we build the 'addition' model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# create model to learn addition\r\n",
    "input1 = Input(shape=image_dim)\r\n",
    "input2 = Input(shape=image_dim)\r\n",
    "enc1_out = encoder1(input1)\r\n",
    "enc2_out = encoder2(input2)\r\n",
    "model_c = Concatenate()([enc1_out,enc2_out])\r\n",
    "model_c = Dense(1000,activation='relu')(model_c)\r\n",
    "\r\n",
    "model_b1 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b2 = Dense(200,activation='relu')(model_c)\r\n",
    "\r\n",
    "model_b1 = Dense(100,activation='relu')(model_b1)\r\n",
    "model_b2 = Dense(100,activation='relu')(model_b2)\r\n",
    "\r\n",
    "units =  Dense(10,activation='softmax',name ='units')(model_b1)\r\n",
    "tens = Dense(1,activation='sigmoid',name ='tens')(model_b2)\r\n",
    "\r\n",
    "model_addition = Model(inputs=[input1,input2],outputs=[units,tens])\r\n",
    "\r\n",
    "model_addition.compile(optimizer='nadam', loss = ['categorical_crossentropy','binary_crossentropy'], metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model has two inputs (the two handwritten digits images) and two outputs (units and tens of the sum). We will use two different losses due to the nature of the outputs. Note that there is a common hidden layer of 1000 units, and then two branches (one for each output).  \r\n",
    "We need to create datasets to train and test our model. Inputs will be random combinations of handwritten digits. Outputs will be the sums for each combination. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# generate a dataset for additions\r\n",
    "train_size = 200000\r\n",
    "random_labels1 = np.random.randint(0,25000,train_size)\r\n",
    "random_labels2 = np.random.randint(0,25000,train_size)\r\n",
    "\r\n",
    "x_train_1 = x_train[random_labels1]\r\n",
    "x_train_2 = x_train[random_labels2]\r\n",
    "\r\n",
    "y_train_1 = y_train[random_labels1]\r\n",
    "y_train_2 = y_train[random_labels2]\r\n",
    "\r\n",
    "y_add = y_train_1 + y_train_2\r\n",
    "y_add_tens = y_add //10 \r\n",
    "y_add_units = y_add %10 \r\n",
    "y_add_units_cat = to_categorical(y_add_units)\r\n",
    "\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "test_size = 5000\r\n",
    "random_labels1 = np.random.randint(0,10000,test_size)\r\n",
    "random_labels2 = np.random.randint(0,10000,test_size)\r\n",
    "\r\n",
    "x_test_1 = x_test[random_labels1]\r\n",
    "x_test_2 = x_test[random_labels2]\r\n",
    "\r\n",
    "y_test_1 = y_test[random_labels1]\r\n",
    "y_test_2 = y_test[random_labels2]\r\n",
    "\r\n",
    "y_test_add = y_test_1 + y_test_2\r\n",
    "y_test_add_tens = y_test_add //10 \r\n",
    "y_test_add_units = y_test_add %10 \r\n",
    "y_test_add_units_cat = to_categorical(y_test_add_units)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to train our model! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "history_addition = model_addition.fit([x_train_1,x_train_2],[y_add_units_cat,y_add_tens], batch_size=100,epochs=1000,validation_split=0.2, verbose=2,callbacks=[es,es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "1600/1600 - 32s - loss: 1.2062 - units_loss: 0.9615 - tens_loss: 0.2448 - units_acc: 0.6746 - tens_acc: 0.8922 - val_loss: 0.5480 - val_units_loss: 0.4246 - val_tens_loss: 0.1234 - val_units_acc: 0.8713 - val_tens_acc: 0.9549\n",
      "Epoch 2/1000\n",
      "1600/1600 - 29s - loss: 0.4051 - units_loss: 0.3110 - tens_loss: 0.0941 - units_acc: 0.9051 - tens_acc: 0.9653 - val_loss: 0.3432 - val_units_loss: 0.2617 - val_tens_loss: 0.0816 - val_units_acc: 0.9190 - val_tens_acc: 0.9695\n",
      "Epoch 3/1000\n",
      "1600/1600 - 29s - loss: 0.2549 - units_loss: 0.1957 - tens_loss: 0.0592 - units_acc: 0.9396 - tens_acc: 0.9786 - val_loss: 0.2623 - val_units_loss: 0.1953 - val_tens_loss: 0.0670 - val_units_acc: 0.9401 - val_tens_acc: 0.9749\n",
      "Epoch 4/1000\n",
      "1600/1600 - 32s - loss: 0.1781 - units_loss: 0.1367 - tens_loss: 0.0413 - units_acc: 0.9570 - tens_acc: 0.9849 - val_loss: 0.2058 - val_units_loss: 0.1577 - val_tens_loss: 0.0481 - val_units_acc: 0.9503 - val_tens_acc: 0.9823\n",
      "Epoch 5/1000\n",
      "1600/1600 - 29s - loss: 0.1319 - units_loss: 0.1025 - tens_loss: 0.0294 - units_acc: 0.9674 - tens_acc: 0.9893 - val_loss: 0.1854 - val_units_loss: 0.1428 - val_tens_loss: 0.0426 - val_units_acc: 0.9550 - val_tens_acc: 0.9843\n",
      "Epoch 6/1000\n",
      "1600/1600 - 30s - loss: 0.1023 - units_loss: 0.0789 - tens_loss: 0.0234 - units_acc: 0.9737 - tens_acc: 0.9914 - val_loss: 0.1459 - val_units_loss: 0.1107 - val_tens_loss: 0.0352 - val_units_acc: 0.9659 - val_tens_acc: 0.9876\n",
      "Epoch 7/1000\n",
      "1600/1600 - 32s - loss: 0.0835 - units_loss: 0.0651 - tens_loss: 0.0184 - units_acc: 0.9787 - tens_acc: 0.9933 - val_loss: 0.1337 - val_units_loss: 0.1021 - val_tens_loss: 0.0316 - val_units_acc: 0.9679 - val_tens_acc: 0.9890\n",
      "Epoch 8/1000\n",
      "1600/1600 - 29s - loss: 0.0725 - units_loss: 0.0563 - tens_loss: 0.0162 - units_acc: 0.9813 - tens_acc: 0.9942 - val_loss: 0.1440 - val_units_loss: 0.1073 - val_tens_loss: 0.0367 - val_units_acc: 0.9671 - val_tens_acc: 0.9881\n",
      "Epoch 9/1000\n",
      "1600/1600 - 29s - loss: 0.0623 - units_loss: 0.0487 - tens_loss: 0.0136 - units_acc: 0.9838 - tens_acc: 0.9952 - val_loss: 0.1145 - val_units_loss: 0.0880 - val_tens_loss: 0.0265 - val_units_acc: 0.9747 - val_tens_acc: 0.9916\n",
      "Epoch 10/1000\n",
      "1600/1600 - 29s - loss: 0.0570 - units_loss: 0.0443 - tens_loss: 0.0127 - units_acc: 0.9852 - tens_acc: 0.9954 - val_loss: 0.1104 - val_units_loss: 0.0838 - val_tens_loss: 0.0266 - val_units_acc: 0.9759 - val_tens_acc: 0.9913\n",
      "Epoch 11/1000\n",
      "1600/1600 - 29s - loss: 0.0486 - units_loss: 0.0381 - tens_loss: 0.0105 - units_acc: 0.9872 - tens_acc: 0.9962 - val_loss: 0.1217 - val_units_loss: 0.0893 - val_tens_loss: 0.0324 - val_units_acc: 0.9742 - val_tens_acc: 0.9898\n",
      "Epoch 12/1000\n",
      "1600/1600 - 29s - loss: 0.0461 - units_loss: 0.0360 - tens_loss: 0.0101 - units_acc: 0.9880 - tens_acc: 0.9967 - val_loss: 0.1138 - val_units_loss: 0.0833 - val_tens_loss: 0.0305 - val_units_acc: 0.9751 - val_tens_acc: 0.9904\n",
      "Epoch 13/1000\n",
      "1600/1600 - 29s - loss: 0.0441 - units_loss: 0.0344 - tens_loss: 0.0097 - units_acc: 0.9886 - tens_acc: 0.9966 - val_loss: 0.1336 - val_units_loss: 0.0990 - val_tens_loss: 0.0345 - val_units_acc: 0.9719 - val_tens_acc: 0.9895\n",
      "Epoch 14/1000\n",
      "1600/1600 - 30s - loss: 0.0390 - units_loss: 0.0304 - tens_loss: 0.0086 - units_acc: 0.9901 - tens_acc: 0.9972 - val_loss: 0.1017 - val_units_loss: 0.0766 - val_tens_loss: 0.0251 - val_units_acc: 0.9776 - val_tens_acc: 0.9921\n",
      "Epoch 15/1000\n",
      "1600/1600 - 29s - loss: 0.0398 - units_loss: 0.0314 - tens_loss: 0.0084 - units_acc: 0.9892 - tens_acc: 0.9972 - val_loss: 0.1147 - val_units_loss: 0.0855 - val_tens_loss: 0.0292 - val_units_acc: 0.9761 - val_tens_acc: 0.9911\n",
      "Epoch 16/1000\n",
      "1600/1600 - 29s - loss: 0.0324 - units_loss: 0.0254 - tens_loss: 0.0071 - units_acc: 0.9915 - tens_acc: 0.9976 - val_loss: 0.1070 - val_units_loss: 0.0790 - val_tens_loss: 0.0279 - val_units_acc: 0.9780 - val_tens_acc: 0.9906\n",
      "Epoch 17/1000\n",
      "1600/1600 - 29s - loss: 0.0332 - units_loss: 0.0265 - tens_loss: 0.0067 - units_acc: 0.9912 - tens_acc: 0.9977 - val_loss: 0.0882 - val_units_loss: 0.0661 - val_tens_loss: 0.0221 - val_units_acc: 0.9816 - val_tens_acc: 0.9935\n",
      "Epoch 18/1000\n",
      "1600/1600 - 29s - loss: 0.0307 - units_loss: 0.0242 - tens_loss: 0.0064 - units_acc: 0.9922 - tens_acc: 0.9978 - val_loss: 0.1062 - val_units_loss: 0.0800 - val_tens_loss: 0.0262 - val_units_acc: 0.9776 - val_tens_acc: 0.9922\n",
      "Epoch 19/1000\n",
      "1600/1600 - 29s - loss: 0.0328 - units_loss: 0.0260 - tens_loss: 0.0068 - units_acc: 0.9913 - tens_acc: 0.9975 - val_loss: 0.1073 - val_units_loss: 0.0815 - val_tens_loss: 0.0258 - val_units_acc: 0.9777 - val_tens_acc: 0.9924\n",
      "Epoch 20/1000\n",
      "1600/1600 - 29s - loss: 0.0294 - units_loss: 0.0231 - tens_loss: 0.0064 - units_acc: 0.9925 - tens_acc: 0.9978 - val_loss: 0.1034 - val_units_loss: 0.0744 - val_tens_loss: 0.0290 - val_units_acc: 0.9801 - val_tens_acc: 0.9919\n",
      "Epoch 21/1000\n",
      "1600/1600 - 29s - loss: 0.0273 - units_loss: 0.0217 - tens_loss: 0.0056 - units_acc: 0.9927 - tens_acc: 0.9982 - val_loss: 0.0985 - val_units_loss: 0.0719 - val_tens_loss: 0.0266 - val_units_acc: 0.9809 - val_tens_acc: 0.9925\n",
      "Epoch 22/1000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1600/1600 - 29s - loss: 0.0262 - units_loss: 0.0211 - tens_loss: 0.0051 - units_acc: 0.9932 - tens_acc: 0.9982 - val_loss: 0.1014 - val_units_loss: 0.0799 - val_tens_loss: 0.0215 - val_units_acc: 0.9799 - val_tens_acc: 0.9942\n",
      "Epoch 00022: early stopping\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a03037eb48>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of the training, the accuracy on both outputs is pretty good (98% and 99,5%). Let's see first how the model performs on the test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "test_results = model_addition.evaluate([x_test_1,x_test_2],[y_test_add_units_cat,y_test_add_tens],batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5/5 [==============================] - 0s 51ms/step - loss: 0.4157 - units_loss: 0.3104 - tens_loss: 0.1052 - units_acc: 0.9398 - tens_acc: 0.9806\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results are still in the 9x%. We can show a random sample of the model predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "random_label_1 = np.random.randint(0,9999)\r\n",
    "random_label_2 = np.random.randint(0,9999)\r\n",
    "\r\n",
    "img_sample1 = x_test[random_label_1,:,:].reshape((1,28,28,1))\r\n",
    "img_sample2 = x_test[random_label_2,:,:].reshape((1,28,28,1))\r\n",
    "\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.imshow(img_sample1.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.imshow(img_sample2.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "prediction = model_addition.predict([img_sample1,img_sample2])\r\n",
    "unit = prediction[0]\r\n",
    "ten = prediction[1]\r\n",
    "\r\n",
    "sum_images = np.argmax(unit)+10*np.round(ten)\r\n",
    "print('sum =',sum_images)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sum = [[4.]]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results look promising! We actually could improve the accuracy by training the model on more random samples (increase `train_size` value) or tweak the model architecture. One last thing: save the model!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# save the model\r\n",
    "model_addition.save('model_addition')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning how to multiply two handwritten digits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using a similar method, we can train a neural network to compute multiplication result of two handwritten digits. The main difference is that the output will be in the range [0,81]. The network will output two values:  \r\n",
    "1- units, multiclass [0,1,2,3,4,5,6,7,8,9]  \r\n",
    "2- tens, multiclass [0,1,2,3,4,5,6,7,8] \r\n",
    "\r\n",
    "We will use the same architecture as previously, with a slight modification in the output layer (softmax instead of sigmoid, and 8 neurons instead of 1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# duplicate encoders and freeze weights\r\n",
    "encoder3 = keras.models.load_model('encoder') \r\n",
    "encoder3._name = 'encoder1'\r\n",
    "encoder3.trainable = False\r\n",
    "\r\n",
    "encoder4 = keras.models.load_model('encoder')\r\n",
    "encoder4._name = 'encoder2'\r\n",
    "encoder4.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# create model to learn multiplication\r\n",
    "input1 = Input(shape=image_dim)\r\n",
    "input2 = Input(shape=image_dim)\r\n",
    "enc1_out = encoder3(input1)\r\n",
    "enc2_out = encoder4(input2)\r\n",
    "model_c = Concatenate()([enc1_out,enc2_out])\r\n",
    "model_c = Dense(1000,activation='relu')(model_c)\r\n",
    "\r\n",
    "model_b1 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b2 = Dense(200,activation='relu')(model_c)\r\n",
    "\r\n",
    "model_b1 = Dense(100,activation='relu')(model_b1)\r\n",
    "model_b2 = Dense(100,activation='relu')(model_b2)\r\n",
    "\r\n",
    "units =  Dense(10,activation='softmax',name ='units')(model_b1)\r\n",
    "tens = Dense(9,activation='softmax',name ='tens')(model_b2)\r\n",
    "\r\n",
    "model_mult = Model(inputs=[input1,input2],outputs=[units,tens])\r\n",
    "\r\n",
    "model_mult.compile(optimizer='nadam', loss = ['categorical_crossentropy','categorical_crossentropy'], metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to create data for training and testing as we did previously. We already generated random images, so all we need now is to create labels by multiplying the values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# generate a dataset for multiplication\r\n",
    "\r\n",
    "y_mult = y_train_1 * y_train_2\r\n",
    "y_mult_tens = y_mult //10 \r\n",
    "y_mult_units = y_mult %10 \r\n",
    "y_mult_units_cat = to_categorical(y_mult_units)\r\n",
    "y_mult_tens_cat = to_categorical(y_mult_tens)\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "\r\n",
    "y_test_mult = y_test_1 * y_test_2\r\n",
    "y_test_mult_tens = y_test_mult //10 \r\n",
    "y_test_mult_units = y_test_mult %10 \r\n",
    "y_test_mult_units_cat = to_categorical(y_test_mult_units)\r\n",
    "y_test_mult_tens_cat = to_categorical(y_test_mult_tens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next step is to train the model, and test it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "history_mult = model_mult.fit([x_train_1,x_train_2],[y_mult_units_cat,y_mult_tens_cat], batch_size=100,epochs=1000,validation_split=0.2, verbose=2,callbacks=[es,es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "1600/1600 - 31s - loss: 1.2452 - units_loss: 0.6768 - tens_loss: 0.5684 - units_acc: 0.7661 - tens_acc: 0.7980 - val_loss: 0.5906 - val_units_loss: 0.3276 - val_tens_loss: 0.2630 - val_units_acc: 0.8949 - val_tens_acc: 0.9129\n",
      "Epoch 2/1000\n",
      "1600/1600 - 30s - loss: 0.4313 - units_loss: 0.2278 - tens_loss: 0.2034 - units_acc: 0.9279 - tens_acc: 0.9327 - val_loss: 0.3499 - val_units_loss: 0.1837 - val_tens_loss: 0.1662 - val_units_acc: 0.9413 - val_tens_acc: 0.9459\n",
      "Epoch 3/1000\n",
      "1600/1600 - 29s - loss: 0.2800 - units_loss: 0.1479 - tens_loss: 0.1321 - units_acc: 0.9523 - tens_acc: 0.9569 - val_loss: 0.2667 - val_units_loss: 0.1405 - val_tens_loss: 0.1262 - val_units_acc: 0.9544 - val_tens_acc: 0.9579\n",
      "Epoch 4/1000\n",
      "1600/1600 - 30s - loss: 0.2003 - units_loss: 0.1051 - tens_loss: 0.0952 - units_acc: 0.9653 - tens_acc: 0.9683 - val_loss: 0.2363 - val_units_loss: 0.1202 - val_tens_loss: 0.1161 - val_units_acc: 0.9620 - val_tens_acc: 0.9636\n",
      "Epoch 5/1000\n",
      "1600/1600 - 31s - loss: 0.1500 - units_loss: 0.0783 - tens_loss: 0.0717 - units_acc: 0.9746 - tens_acc: 0.9758 - val_loss: 0.2043 - val_units_loss: 0.1018 - val_tens_loss: 0.1025 - val_units_acc: 0.9682 - val_tens_acc: 0.9657\n",
      "Epoch 6/1000\n",
      "1600/1600 - 31s - loss: 0.1183 - units_loss: 0.0619 - tens_loss: 0.0564 - units_acc: 0.9793 - tens_acc: 0.9801 - val_loss: 0.1833 - val_units_loss: 0.0901 - val_tens_loss: 0.0932 - val_units_acc: 0.9706 - val_tens_acc: 0.9699\n",
      "Epoch 7/1000\n",
      "1600/1600 - 31s - loss: 0.0994 - units_loss: 0.0520 - tens_loss: 0.0475 - units_acc: 0.9827 - tens_acc: 0.9838 - val_loss: 0.1609 - val_units_loss: 0.0829 - val_tens_loss: 0.0781 - val_units_acc: 0.9740 - val_tens_acc: 0.9757\n",
      "Epoch 8/1000\n",
      "1600/1600 - 31s - loss: 0.0837 - units_loss: 0.0446 - tens_loss: 0.0391 - units_acc: 0.9849 - tens_acc: 0.9864 - val_loss: 0.1552 - val_units_loss: 0.0802 - val_tens_loss: 0.0750 - val_units_acc: 0.9764 - val_tens_acc: 0.9765\n",
      "Epoch 9/1000\n",
      "1600/1600 - 29s - loss: 0.0734 - units_loss: 0.0386 - tens_loss: 0.0348 - units_acc: 0.9869 - tens_acc: 0.9880 - val_loss: 0.1380 - val_units_loss: 0.0685 - val_tens_loss: 0.0696 - val_units_acc: 0.9786 - val_tens_acc: 0.9782\n",
      "Epoch 10/1000\n",
      "1600/1600 - 28s - loss: 0.0612 - units_loss: 0.0317 - tens_loss: 0.0295 - units_acc: 0.9894 - tens_acc: 0.9900 - val_loss: 0.1225 - val_units_loss: 0.0653 - val_tens_loss: 0.0573 - val_units_acc: 0.9801 - val_tens_acc: 0.9824\n",
      "Epoch 11/1000\n",
      "1600/1600 - 28s - loss: 0.0598 - units_loss: 0.0318 - tens_loss: 0.0280 - units_acc: 0.9894 - tens_acc: 0.9900 - val_loss: 0.1275 - val_units_loss: 0.0649 - val_tens_loss: 0.0626 - val_units_acc: 0.9794 - val_tens_acc: 0.9798\n",
      "Epoch 12/1000\n",
      "1600/1600 - 30s - loss: 0.0533 - units_loss: 0.0281 - tens_loss: 0.0252 - units_acc: 0.9902 - tens_acc: 0.9912 - val_loss: 0.1196 - val_units_loss: 0.0634 - val_tens_loss: 0.0562 - val_units_acc: 0.9807 - val_tens_acc: 0.9820\n",
      "Epoch 13/1000\n",
      "1600/1600 - 30s - loss: 0.0471 - units_loss: 0.0248 - tens_loss: 0.0223 - units_acc: 0.9916 - tens_acc: 0.9924 - val_loss: 0.1462 - val_units_loss: 0.0780 - val_tens_loss: 0.0682 - val_units_acc: 0.9780 - val_tens_acc: 0.9805\n",
      "Epoch 14/1000\n",
      "1600/1600 - 29s - loss: 0.0434 - units_loss: 0.0229 - tens_loss: 0.0205 - units_acc: 0.9925 - tens_acc: 0.9931 - val_loss: 0.1304 - val_units_loss: 0.0649 - val_tens_loss: 0.0655 - val_units_acc: 0.9812 - val_tens_acc: 0.9809\n",
      "Epoch 15/1000\n",
      "1600/1600 - 29s - loss: 0.0435 - units_loss: 0.0226 - tens_loss: 0.0209 - units_acc: 0.9925 - tens_acc: 0.9931 - val_loss: 0.1106 - val_units_loss: 0.0592 - val_tens_loss: 0.0514 - val_units_acc: 0.9827 - val_tens_acc: 0.9837\n",
      "Epoch 16/1000\n",
      "1600/1600 - 29s - loss: 0.0379 - units_loss: 0.0200 - tens_loss: 0.0179 - units_acc: 0.9932 - tens_acc: 0.9939 - val_loss: 0.1114 - val_units_loss: 0.0586 - val_tens_loss: 0.0528 - val_units_acc: 0.9827 - val_tens_acc: 0.9849\n",
      "Epoch 17/1000\n",
      "1600/1600 - 28s - loss: 0.0380 - units_loss: 0.0200 - tens_loss: 0.0180 - units_acc: 0.9932 - tens_acc: 0.9941 - val_loss: 0.1007 - val_units_loss: 0.0533 - val_tens_loss: 0.0474 - val_units_acc: 0.9851 - val_tens_acc: 0.9869\n",
      "Epoch 18/1000\n",
      "1600/1600 - 29s - loss: 0.0333 - units_loss: 0.0179 - tens_loss: 0.0154 - units_acc: 0.9940 - tens_acc: 0.9949 - val_loss: 0.1065 - val_units_loss: 0.0540 - val_tens_loss: 0.0525 - val_units_acc: 0.9845 - val_tens_acc: 0.9853\n",
      "Epoch 19/1000\n",
      "1600/1600 - 30s - loss: 0.0346 - units_loss: 0.0178 - tens_loss: 0.0168 - units_acc: 0.9940 - tens_acc: 0.9944 - val_loss: 0.1086 - val_units_loss: 0.0569 - val_tens_loss: 0.0517 - val_units_acc: 0.9848 - val_tens_acc: 0.9855\n",
      "Epoch 20/1000\n",
      "1600/1600 - 30s - loss: 0.0315 - units_loss: 0.0165 - tens_loss: 0.0150 - units_acc: 0.9947 - tens_acc: 0.9949 - val_loss: 0.1352 - val_units_loss: 0.0708 - val_tens_loss: 0.0644 - val_units_acc: 0.9814 - val_tens_acc: 0.9818\n",
      "Epoch 21/1000\n",
      "1600/1600 - 30s - loss: 0.0300 - units_loss: 0.0160 - tens_loss: 0.0140 - units_acc: 0.9950 - tens_acc: 0.9955 - val_loss: 0.1064 - val_units_loss: 0.0582 - val_tens_loss: 0.0482 - val_units_acc: 0.9843 - val_tens_acc: 0.9863\n",
      "Epoch 22/1000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1600/1600 - 30s - loss: 0.0288 - units_loss: 0.0152 - tens_loss: 0.0136 - units_acc: 0.9951 - tens_acc: 0.9954 - val_loss: 0.1575 - val_units_loss: 0.0798 - val_tens_loss: 0.0777 - val_units_acc: 0.9800 - val_tens_acc: 0.9797\n",
      "Epoch 00022: early stopping\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a0365dac48>"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "test_results = model_mult.evaluate([x_test_1,x_test_2],[y_test_mult_units_cat,y_test_mult_tens_cat],batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4278 - units_loss: 0.2224 - tens_loss: 0.2053 - units_acc: 0.9544 - tens_acc: 0.9602\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We achieve similar performances (slightly better actually!) when compared to addition. Let's see how the model works on sample data: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "random_label_1 = np.random.randint(0,9999)\r\n",
    "random_label_2 = np.random.randint(0,9999)\r\n",
    "\r\n",
    "img_sample1 = x_test[random_label_1,:,:].reshape((1,28,28,1))\r\n",
    "img_sample2 = x_test[random_label_2,:,:].reshape((1,28,28,1))\r\n",
    "\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.imshow(img_sample1.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.imshow(img_sample2.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "prediction = model_mult.predict([img_sample1,img_sample2])\r\n",
    "unit = prediction[0]\r\n",
    "ten = prediction[1]\r\n",
    "\r\n",
    "mult_images = np.argmax(unit)+10*np.argmax(ten)\r\n",
    "print('multiplication result =',mult_images)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "multiplication result = 63\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# save the model\r\n",
    "model_mult.save('model_mult')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning how to compare two handwritten digits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last arithmetic operation our model will predict is the comparison. The model will have one binary output (1 if image1 > image2 and 0 elsewhere). We proceed the same way as previously."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# duplicate encoders and freeze weights\r\n",
    "encoder5 = keras.models.load_model('encoder') \r\n",
    "encoder5._name = 'encoder1'\r\n",
    "encoder5.trainable = False\r\n",
    "\r\n",
    "encoder6 = keras.models.load_model('encoder')\r\n",
    "encoder6._name = 'encoder2'\r\n",
    "encoder6.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# create model to learn comparison\r\n",
    "input1 = Input(shape=image_dim)\r\n",
    "input2 = Input(shape=image_dim)\r\n",
    "enc1_out = encoder5(input1)\r\n",
    "enc2_out = encoder6(input2)\r\n",
    "model_c = Concatenate()([enc1_out,enc2_out])\r\n",
    "model_c = Dense(1000,activation='relu')(model_c)\r\n",
    "\r\n",
    "model_c = Dense(200,activation='relu')(model_c)\r\n",
    "\r\n",
    "model_c = Dense(100,activation='relu')(model_c)\r\n",
    "\r\n",
    "comp =  Dense(1,activation='sigmoid',name ='units')(model_c)\r\n",
    "\r\n",
    "\r\n",
    "model_comp = Model(inputs=[input1,input2],outputs=[comp])\r\n",
    "\r\n",
    "model_comp.compile(optimizer='nadam', loss = ['binary_crossentropy'], metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# generate a dataset for comparison\r\n",
    "y_comp = y_train_1 > y_train_2\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "y_test_comp = y_test_1 > y_test_2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "history_comp = model_comp.fit([x_train_1,x_train_2],y_comp, batch_size=100,epochs=1000,validation_split=0.2, verbose=2,callbacks=[es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "1600/1600 - 24s - loss: 0.2903 - acc: 0.8694 - val_loss: 0.1704 - val_acc: 0.9336\n",
      "Epoch 2/1000\n",
      "1600/1600 - 22s - loss: 0.1518 - acc: 0.9406 - val_loss: 0.1260 - val_acc: 0.9513\n",
      "Epoch 3/1000\n",
      "1600/1600 - 23s - loss: 0.1121 - acc: 0.9570 - val_loss: 0.1129 - val_acc: 0.9569\n",
      "Epoch 4/1000\n",
      "1600/1600 - 22s - loss: 0.0887 - acc: 0.9670 - val_loss: 0.0984 - val_acc: 0.9627\n",
      "Epoch 5/1000\n",
      "1600/1600 - 22s - loss: 0.0733 - acc: 0.9727 - val_loss: 0.0882 - val_acc: 0.9678\n",
      "Epoch 6/1000\n",
      "1600/1600 - 22s - loss: 0.0635 - acc: 0.9763 - val_loss: 0.0821 - val_acc: 0.9700\n",
      "Epoch 7/1000\n",
      "1600/1600 - 23s - loss: 0.0554 - acc: 0.9795 - val_loss: 0.0670 - val_acc: 0.9762\n",
      "Epoch 8/1000\n",
      "1600/1600 - 23s - loss: 0.0487 - acc: 0.9820 - val_loss: 0.0719 - val_acc: 0.9739\n",
      "Epoch 9/1000\n",
      "1600/1600 - 24s - loss: 0.0430 - acc: 0.9843 - val_loss: 0.0620 - val_acc: 0.9775\n",
      "Epoch 10/1000\n",
      "1600/1600 - 25s - loss: 0.0385 - acc: 0.9857 - val_loss: 0.0671 - val_acc: 0.9771\n",
      "Epoch 11/1000\n",
      "1600/1600 - 23s - loss: 0.0345 - acc: 0.9874 - val_loss: 0.0683 - val_acc: 0.9771\n",
      "Epoch 12/1000\n",
      "1600/1600 - 23s - loss: 0.0322 - acc: 0.9880 - val_loss: 0.0555 - val_acc: 0.9804\n",
      "Epoch 13/1000\n",
      "1600/1600 - 22s - loss: 0.0284 - acc: 0.9895 - val_loss: 0.0555 - val_acc: 0.9812\n",
      "Epoch 14/1000\n",
      "1600/1600 - 22s - loss: 0.0253 - acc: 0.9907 - val_loss: 0.0598 - val_acc: 0.9806\n",
      "Epoch 15/1000\n",
      "1600/1600 - 22s - loss: 0.0247 - acc: 0.9911 - val_loss: 0.0552 - val_acc: 0.9827\n",
      "Epoch 16/1000\n",
      "1600/1600 - 21s - loss: 0.0217 - acc: 0.9919 - val_loss: 0.0573 - val_acc: 0.9823\n",
      "Epoch 17/1000\n",
      "1600/1600 - 22s - loss: 0.0202 - acc: 0.9927 - val_loss: 0.0622 - val_acc: 0.9799\n",
      "Epoch 18/1000\n",
      "1600/1600 - 21s - loss: 0.0195 - acc: 0.9928 - val_loss: 0.0624 - val_acc: 0.9805\n",
      "Epoch 19/1000\n",
      "1600/1600 - 21s - loss: 0.0177 - acc: 0.9938 - val_loss: 0.0570 - val_acc: 0.9809\n",
      "Epoch 20/1000\n",
      "1600/1600 - 22s - loss: 0.0167 - acc: 0.9940 - val_loss: 0.0566 - val_acc: 0.9818\n",
      "Epoch 21/1000\n",
      "1600/1600 - 22s - loss: 0.0167 - acc: 0.9939 - val_loss: 0.0613 - val_acc: 0.9826\n",
      "Epoch 22/1000\n",
      "1600/1600 - 21s - loss: 0.0154 - acc: 0.9945 - val_loss: 0.0543 - val_acc: 0.9832\n",
      "Epoch 23/1000\n",
      "1600/1600 - 21s - loss: 0.0149 - acc: 0.9948 - val_loss: 0.0521 - val_acc: 0.9852\n",
      "Epoch 24/1000\n",
      "1600/1600 - 22s - loss: 0.0134 - acc: 0.9951 - val_loss: 0.0577 - val_acc: 0.9838\n",
      "Epoch 25/1000\n",
      "1600/1600 - 22s - loss: 0.0139 - acc: 0.9948 - val_loss: 0.0620 - val_acc: 0.9821\n",
      "Epoch 26/1000\n",
      "1600/1600 - 23s - loss: 0.0130 - acc: 0.9952 - val_loss: 0.0533 - val_acc: 0.9846\n",
      "Epoch 27/1000\n",
      "1600/1600 - 22s - loss: 0.0115 - acc: 0.9960 - val_loss: 0.0638 - val_acc: 0.9827\n",
      "Epoch 28/1000\n",
      "1600/1600 - 22s - loss: 0.0133 - acc: 0.9953 - val_loss: 0.0651 - val_acc: 0.9825\n",
      "Epoch 29/1000\n",
      "1600/1600 - 22s - loss: 0.0102 - acc: 0.9963 - val_loss: 0.0560 - val_acc: 0.9854\n",
      "Epoch 30/1000\n",
      "1600/1600 - 22s - loss: 0.0111 - acc: 0.9961 - val_loss: 0.0569 - val_acc: 0.9839\n",
      "Epoch 31/1000\n",
      "1600/1600 - 21s - loss: 0.0114 - acc: 0.9960 - val_loss: 0.0621 - val_acc: 0.9833\n",
      "Epoch 32/1000\n",
      "1600/1600 - 21s - loss: 0.0097 - acc: 0.9965 - val_loss: 0.0645 - val_acc: 0.9830\n",
      "Epoch 33/1000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1600/1600 - 22s - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0584 - val_acc: 0.9845\n",
      "Epoch 00033: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a0368af1c8>"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "test_results = model_comp.evaluate([x_test_1,x_test_2],y_test_comp,batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5/5 [==============================] - 0s 50ms/step - loss: 0.1200 - acc: 0.9700\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "random_label_1 = np.random.randint(0,9999)\r\n",
    "random_label_2 = np.random.randint(0,9999)\r\n",
    "\r\n",
    "img_sample1 = x_test[random_label_1,:,:].reshape((1,28,28,1))\r\n",
    "img_sample2 = x_test[random_label_2,:,:].reshape((1,28,28,1))\r\n",
    "\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.imshow(img_sample1.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.imshow(img_sample2.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "prediction = np.round(model_comp.predict([img_sample1,img_sample2]))\r\n",
    "\r\n",
    "\r\n",
    "print('comparison result =',prediction,'1 if the number on the left is greater, 0 elsewhere')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "comparison result = [[0.]] 1 if the number on the left is greater, 0 elsewhere\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save the model\r\n",
    "model_comp.save('model_comp')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion and future work"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "684b1123683431d89d3bfe9a89cc763215f4b8cd94b4aba1fb40ad45ff7c8b41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}