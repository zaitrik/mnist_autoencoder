{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This is how a neural network learns to add, multiply and compare handwritten digits WITHOUT knowing their values "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\"> <img src=\"https://i.dlpng.com/static/png/6906777_preview.png\"> </p>   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I described in a [previous post](https://blog.jovian.ai/how-to-train-supervised-machine-learning-algorithms-without-labeled-data-6ebddc01a00f), how useful are autoencoders in  automated labeling. The main property of these networks is their ability to learn features/patterns in the data. This is in fact not specific to autoencoders and can be implemented using other unsupervised techniques, mainly **PCA**.  \r\n",
    "The ability to detect and learn features in data can be used in other areas.  \r\n",
    "\r\n",
    "In this post, I will present some applications of convolutional autoencoders:  \r\n",
    "- First, a convolutional autoencoder will be trained on **MNIST** data.\r\n",
    "- After the training of the encoder and decoder, we will freeze their weights and use them with additional dense layers to \"learn\" arithmetic operations, namely addition, multiplication and comparison.  \r\n",
    "The trick is to *never* explicitly associate the handwritten digits in **MNIST** dataset with their respective labels. We will see that the neural networks will be nevertheless able to reach 97+% accuracy in all cases on unseen data.\r\n",
    "\r\n",
    "The first step is described in the following diagram:\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/chLUEdp.png\"> </p>   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the second step, we will use the encoder in series with dense layers to perform arithmetic operations: addition, multiplication and comparison. We will train only the dense layer weights, and supply the results of the operations as labels. note that we will not supply the digits values (labels).\r\n",
    "\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/s8U8up4.png\"> </p> \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training an autoencoder on MNIST data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar to the previous article, we will use MNIST data in this experiment. The autoencoder will learn the handwritten digits features using 60000 training samples. We import MNIST using *KERAS* library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#import libraries and setup \r\n",
    "import keras\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import logging\r\n",
    "logging.getLogger('tensorflow').disabled = True\r\n",
    "from keras.models import Sequential, Model\r\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, UpSampling2D, Reshape, Concatenate, Input\r\n",
    "from keras.callbacks import EarlyStopping\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, restore_best_weights=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import mnist\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n",
    "print(x_train.shape,y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We scale the data in the range `[0,1]` and reshape it to *KERAS* format for pictures (nbr_samples x width x height x channels) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#normalize data\r\n",
    "if x_train.max() >1:\r\n",
    "    x_train = x_train / 255\r\n",
    "    x_test = x_test / 255\r\n",
    "\r\n",
    "default_shape = x_train.shape\r\n",
    "#reshape input data to 1 channel\r\n",
    "x_train = x_train.reshape(-1,default_shape[1],default_shape[2],1)\r\n",
    "x_test = x_test.reshape(-1,default_shape[1],default_shape[2],1)\r\n",
    "image_dim = x_train.shape[1:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will implement a similar autoencoder architecture as in [[1]](https://blog.jovian.ai/how-to-train-supervised-machine-learning-algorithms-without-labeled-data-6ebddc01a00f). It is based on a series of convolutional layers, that will gradually encode the 28x28 image (784 pixel) into a 100 elements array, and decode that representation back to the original format. The resulting image -after the training step- will hopefully resemble to the original one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# create an autoencoder / decoder \r\n",
    "encoder = Sequential()\r\n",
    "encoder.add(Conv2D(32,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu',input_shape=image_dim))\r\n",
    "encoder.add(MaxPooling2D(2,2))\r\n",
    "encoder.add(Conv2D(64,kernel_size=(3,3), strides=(1,1),padding='same',activation='selu'))\r\n",
    "encoder.add(MaxPooling2D(2,2))\r\n",
    "encoder.add(Conv2D(128,kernel_size=(3,3), strides=(1,1),padding='same',activation='selu'))\r\n",
    "encoder.add(Flatten())\r\n",
    "encoder.add(Dense(100,activation='sigmoid'))\r\n",
    "encoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               627300    \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 719,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "encoder_out_dim = encoder.layers[-1].output_shape[1:] # dimension of the encoder output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "decoder = Sequential()\r\n",
    "decoder.add(Dense(6272, activation='sigmoid', input_shape=encoder_out_dim))\r\n",
    "decoder.add(Reshape(( 7, 7, 128)))\r\n",
    "decoder.add(Conv2D(128,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu'))\r\n",
    "decoder.add(UpSampling2D((2,2)))\r\n",
    "decoder.add(Conv2D(64,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu'))\r\n",
    "decoder.add(UpSampling2D((2,2)))\r\n",
    "decoder.add(Conv2D(1,kernel_size=(3,3), strides=(1,1),padding='same', activation='sigmoid'))\r\n",
    "\r\n",
    "decoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "=================================================================\n",
      "Total params: 855,425\n",
      "Trainable params: 855,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The autoencoder is created using the encoder and the decoder:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "enc_dec = Sequential([encoder,decoder])\r\n",
    "enc_dec.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 100)               719972    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 28, 28, 1)         855425    \n",
      "=================================================================\n",
      "Total params: 1,575,397\n",
      "Trainable params: 1,575,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It will be trained as a set of binary classifiers for each pixel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "enc_dec.compile(optimizer='nadam', loss = 'binary_crossentropy')\r\n",
    "history = enc_dec.fit(x_train,x_train, batch_size=100,epochs=1000,validation_split=0.2, verbose=2,callbacks=[es,es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "480/480 - 15s - loss: 0.2821 - val_loss: 0.2675\n",
      "Epoch 2/1000\n",
      "480/480 - 15s - loss: 0.2642 - val_loss: 0.2636\n",
      "Epoch 3/1000\n",
      "480/480 - 14s - loss: 0.2373 - val_loss: 0.1565\n",
      "Epoch 4/1000\n",
      "480/480 - 14s - loss: 0.1306 - val_loss: 0.1181\n",
      "Epoch 5/1000\n",
      "480/480 - 14s - loss: 0.1148 - val_loss: 0.1116\n",
      "Epoch 6/1000\n",
      "480/480 - 14s - loss: 0.1094 - val_loss: 0.1076\n",
      "Epoch 7/1000\n",
      "480/480 - 14s - loss: 0.1061 - val_loss: 0.1061\n",
      "Epoch 8/1000\n",
      "480/480 - 14s - loss: 0.1041 - val_loss: 0.1033\n",
      "Epoch 9/1000\n",
      "480/480 - 14s - loss: 0.1023 - val_loss: 0.1029\n",
      "Epoch 10/1000\n",
      "480/480 - 14s - loss: 0.1009 - val_loss: 0.1014\n",
      "Epoch 11/1000\n",
      "480/480 - 14s - loss: 0.0999 - val_loss: 0.1003\n",
      "Epoch 12/1000\n",
      "480/480 - 14s - loss: 0.0991 - val_loss: 0.1013\n",
      "Epoch 13/1000\n",
      "480/480 - 15s - loss: 0.0983 - val_loss: 0.1003\n",
      "Epoch 14/1000\n",
      "480/480 - 15s - loss: 0.0976 - val_loss: 0.0993\n",
      "Epoch 15/1000\n",
      "480/480 - 14s - loss: 0.0969 - val_loss: 0.0977\n",
      "Epoch 16/1000\n",
      "480/480 - 14s - loss: 0.0964 - val_loss: 0.0974\n",
      "Epoch 17/1000\n",
      "480/480 - 14s - loss: 0.0960 - val_loss: 0.0968\n",
      "Epoch 18/1000\n",
      "480/480 - 15s - loss: 0.0953 - val_loss: 0.0964\n",
      "Epoch 19/1000\n",
      "480/480 - 15s - loss: 0.0948 - val_loss: 0.0961\n",
      "Epoch 20/1000\n",
      "480/480 - 15s - loss: 0.0942 - val_loss: 0.0955\n",
      "Epoch 21/1000\n",
      "480/480 - 15s - loss: 0.0936 - val_loss: 0.0949\n",
      "Epoch 22/1000\n",
      "480/480 - 14s - loss: 0.0932 - val_loss: 0.0947\n",
      "Epoch 23/1000\n",
      "480/480 - 15s - loss: 0.0929 - val_loss: 0.0943\n",
      "Epoch 24/1000\n",
      "480/480 - 15s - loss: 0.0924 - val_loss: 0.0938\n",
      "Epoch 25/1000\n",
      "480/480 - 15s - loss: 0.0921 - val_loss: 0.0939\n",
      "Epoch 26/1000\n",
      "480/480 - 15s - loss: 0.0916 - val_loss: 0.0938\n",
      "Epoch 27/1000\n",
      "480/480 - 15s - loss: 0.0914 - val_loss: 0.0937\n",
      "Epoch 28/1000\n",
      "480/480 - 15s - loss: 0.0912 - val_loss: 0.0934\n",
      "Epoch 29/1000\n",
      "480/480 - 14s - loss: 0.0909 - val_loss: 0.0940\n",
      "Epoch 30/1000\n",
      "480/480 - 15s - loss: 0.0905 - val_loss: 0.0928\n",
      "Epoch 31/1000\n",
      "480/480 - 15s - loss: 0.0902 - val_loss: 0.0926\n",
      "Epoch 32/1000\n",
      "480/480 - 15s - loss: 0.0900 - val_loss: 0.0924\n",
      "Epoch 33/1000\n",
      "480/480 - 15s - loss: 0.0896 - val_loss: 0.0921\n",
      "Epoch 34/1000\n",
      "480/480 - 15s - loss: 0.0894 - val_loss: 0.0921\n",
      "Epoch 35/1000\n",
      "480/480 - 15s - loss: 0.0894 - val_loss: 0.0920\n",
      "Epoch 36/1000\n",
      "480/480 - 15s - loss: 0.0891 - val_loss: 0.0918\n",
      "Epoch 37/1000\n",
      "480/480 - 16s - loss: 0.0888 - val_loss: 0.0913\n",
      "Epoch 38/1000\n",
      "480/480 - 15s - loss: 0.0886 - val_loss: 0.0914\n",
      "Epoch 39/1000\n",
      "480/480 - 15s - loss: 0.0884 - val_loss: 0.0913\n",
      "Epoch 40/1000\n",
      "480/480 - 15s - loss: 0.0881 - val_loss: 0.0911\n",
      "Epoch 41/1000\n",
      "480/480 - 15s - loss: 0.0880 - val_loss: 0.0913\n",
      "Epoch 42/1000\n",
      "480/480 - 15s - loss: 0.0879 - val_loss: 0.0910\n",
      "Epoch 43/1000\n",
      "480/480 - 14s - loss: 0.0876 - val_loss: 0.0913\n",
      "Epoch 44/1000\n",
      "480/480 - 14s - loss: 0.0874 - val_loss: 0.0907\n",
      "Epoch 45/1000\n",
      "480/480 - 14s - loss: 0.0872 - val_loss: 0.0905\n",
      "Epoch 46/1000\n",
      "480/480 - 14s - loss: 0.0871 - val_loss: 0.0904\n",
      "Epoch 47/1000\n",
      "480/480 - 14s - loss: 0.0869 - val_loss: 0.0905\n",
      "Epoch 48/1000\n",
      "480/480 - 14s - loss: 0.0868 - val_loss: 0.0904\n",
      "Epoch 49/1000\n",
      "480/480 - 14s - loss: 0.0866 - val_loss: 0.0904\n",
      "Epoch 50/1000\n",
      "480/480 - 14s - loss: 0.0866 - val_loss: 0.0903\n",
      "Epoch 51/1000\n",
      "480/480 - 14s - loss: 0.0864 - val_loss: 0.0901\n",
      "Epoch 52/1000\n",
      "480/480 - 14s - loss: 0.0862 - val_loss: 0.0903\n",
      "Epoch 53/1000\n",
      "480/480 - 14s - loss: 0.0861 - val_loss: 0.0902\n",
      "Epoch 54/1000\n",
      "480/480 - 14s - loss: 0.0860 - val_loss: 0.0900\n",
      "Epoch 55/1000\n",
      "480/480 - 14s - loss: 0.0859 - val_loss: 0.0899\n",
      "Epoch 56/1000\n",
      "480/480 - 14s - loss: 0.0858 - val_loss: 0.0900\n",
      "Epoch 57/1000\n",
      "480/480 - 14s - loss: 0.0858 - val_loss: 0.0899\n",
      "Epoch 58/1000\n",
      "480/480 - 14s - loss: 0.0854 - val_loss: 0.0899\n",
      "Epoch 59/1000\n",
      "480/480 - 14s - loss: 0.0854 - val_loss: 0.0901\n",
      "Epoch 60/1000\n",
      "480/480 - 14s - loss: 0.0852 - val_loss: 0.0898\n",
      "Epoch 61/1000\n",
      "480/480 - 14s - loss: 0.0850 - val_loss: 0.0898\n",
      "Epoch 62/1000\n",
      "480/480 - 14s - loss: 0.0849 - val_loss: 0.0898\n",
      "Epoch 63/1000\n",
      "480/480 - 14s - loss: 0.0848 - val_loss: 0.0895\n",
      "Epoch 64/1000\n",
      "480/480 - 14s - loss: 0.0847 - val_loss: 0.0893\n",
      "Epoch 65/1000\n",
      "480/480 - 15s - loss: 0.0846 - val_loss: 0.0899\n",
      "Epoch 66/1000\n",
      "480/480 - 14s - loss: 0.0846 - val_loss: 0.0895\n",
      "Epoch 67/1000\n",
      "480/480 - 14s - loss: 0.0846 - val_loss: 0.0897\n",
      "Epoch 68/1000\n",
      "480/480 - 14s - loss: 0.0845 - val_loss: 0.0896\n",
      "Epoch 69/1000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "480/480 - 14s - loss: 0.0844 - val_loss: 0.0894\n",
      "Epoch 00069: early stopping\n",
      "Epoch 00069: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The early stopping will make sure the autoencoder will not overfit the training data. There are two ways to verify the network. First, we can evaluate the loss function on test data, and expect it to be close to the loss value on the training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "enc_dec.evaluate(x_test,x_test,batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10/10 [==============================] - 1s 71ms/step - loss: 0.0882\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.08822797983884811"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "enc_dec.evaluate(x_train,x_train,batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0852\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.08517985045909882"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is very close, around `0.08` for both data sets. The second method is to check the resulting reconstitution that we obtain for a random sample from the test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "random_label = np.random.randint(0,9999)\r\n",
    "img_sample = x_test[random_label,:,:].reshape((1,28,28,1))\r\n",
    "plt.imshow(img_sample.reshape(28,28), cmap='gray');\r\n",
    "pred_img = enc_dec.predict(img_sample) \r\n",
    "plt.figure();\r\n",
    "plt.imshow(pred_img.reshape(28,28), cmap='gray');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*A picture is worth a thousand words!*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# save models\r\n",
    "encoder.save('encoder')\r\n",
    "decoder.save('decoder')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a trained encoder and decoder, let's focus on the *encoder*. For each image, is associated a representation that captures most of the interesting features. This representation is sufficient to reconstitute the image using the decoder. Here is the representation of the sample image we used earlier: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "representation_sample = encoder.predict(img_sample)\r\n",
    "print(representation_sample) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0000000e+00 1.6840684e-17\n",
      "  1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7144099e-35 1.0000000e+00\n",
      "  1.0000000e+00 6.2723314e-36 1.0000000e+00 1.5508051e-11 1.0000000e+00\n",
      "  0.0000000e+00 1.0364077e-03 1.2028039e-09 0.0000000e+00 5.5973729e-08\n",
      "  0.0000000e+00 3.0960938e-13 0.0000000e+00 0.0000000e+00 1.1984192e-08\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+00 1.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+00 0.0000000e+00 1.0000000e+00 1.0000000e+00 0.0000000e+00\n",
      "  3.9332832e-11 1.0000000e+00 0.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+00 0.0000000e+00 1.0000000e+00 1.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1253273e-06\n",
      "  3.9418192e-24 0.0000000e+00 8.3602914e-15 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 0.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 1.4922305e-38 0.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  3.9187780e-15 1.0000000e+00 0.0000000e+00 1.0951785e-05 1.0000000e+00\n",
      "  7.9969609e-26 1.5429734e-09 1.0000000e+00 1.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 0.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5730533e-37 0.0000000e+00]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using these 100 numbers, we generate a 28x28 image (784 pixels)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "recons_image = decoder.predict(representation_sample)\r\n",
    "plt.imshow(recons_image.reshape(28,28), cmap='gray');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And here is where the *fun part* begins! using the lower-dimension representation, let's do some math."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning arithmetic operations on handwritten digits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The idea is simple. Using the representation of two images, we train a neural network to compute their sum, product and to compare them. We will not provide the value of each digit, but we will provide the results during the training step.  \r\n",
    "We will be performing addition and multiplication between numbers in the range [0-9]. The results will be in the range [0-18] and [0-81] respectively. So the results will be coded using multiple outputs:  \r\n",
    "1- Sum Units, multiclass output [0,1,2,3,4,5,6,7,8,9]  \r\n",
    "2- Sum tens, binary output [0,1]  \r\n",
    "3- Multiplication Units, multiclass output [0,1,2,3,4,5,6,7,8,9]  \r\n",
    "4- Multiplication tens, multiclass output [0,1,2,3,4,5,6,7,8]  \r\n",
    "5- Comparison, binary output [0,1]  \r\n",
    "\r\n",
    "\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/E5unC7Y.png\"> </p> \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the functional API in *KERAS* we define the network architecture. First, we import the encoder *twice* and freeze its weights:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# duplicate encoders and freeze weights\r\n",
    "encoder1 = keras.models.load_model('encoder') \r\n",
    "encoder1._name = 'encoder1'\r\n",
    "encoder1.trainable = False\r\n",
    "\r\n",
    "encoder2 = keras.models.load_model('encoder')\r\n",
    "encoder2._name = 'encoder2'\r\n",
    "encoder2.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the encoders, we build the model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# create model to learn it all\r\n",
    "input1 = Input(shape=image_dim)\r\n",
    "input2 = Input(shape=image_dim)\r\n",
    "enc1_out = encoder1(input1)\r\n",
    "enc2_out = encoder2(input2)\r\n",
    "model_c = Concatenate()([enc1_out,enc2_out])\r\n",
    "model_c = Dense(1000,activation='relu')(model_c)\r\n",
    "\r\n",
    "model_b1 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b2 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b3 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b4 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b5 = Dense(200,activation='relu')(model_c)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "units_add =  Dense(10,activation='softmax',name ='units_add')(model_b1)\r\n",
    "tens_add = Dense(1,activation='sigmoid',name ='tens_add')(model_b2)\r\n",
    "\r\n",
    "units_mult =  Dense(10,activation='softmax',name ='units_mult')(model_b3)\r\n",
    "tens_mult = Dense(9,activation='softmax',name ='tens_mult')(model_b4)\r\n",
    "\r\n",
    "comp =  Dense(1,activation='sigmoid',name ='comp')(model_b5)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "model_complete = Model(inputs=[input1,input2],outputs=[units_add,tens_add,units_mult,tens_mult,comp])\r\n",
    "\r\n",
    "model_complete.compile(optimizer='nadam', loss = ['categorical_crossentropy','binary_crossentropy','categorical_crossentropy','categorical_crossentropy','binary_crossentropy'], metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model has two inputs (the two handwritten digits images) and five outputs (units and tens of the sum and product plus the comparsion result). We will use two different losses due to the nature of the outputs. Note that there is a common hidden layer of 1000 units, and then five branches (one for each output).  \r\n",
    "We need to create datasets to train and test our model. Inputs will be random combinations of handwritten digits. Outputs will be the expected results for each combination. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# generate a dataset for additions\r\n",
    "train_size = 500000\r\n",
    "random_labels1 = np.random.randint(0,25000,train_size)\r\n",
    "random_labels2 = np.random.randint(0,25000,train_size)\r\n",
    "\r\n",
    "x_train_1 = x_train[random_labels1]\r\n",
    "x_train_2 = x_train[random_labels2]\r\n",
    "\r\n",
    "y_train_1 = y_train[random_labels1]\r\n",
    "y_train_2 = y_train[random_labels2]\r\n",
    "\r\n",
    "y_add = y_train_1 + y_train_2\r\n",
    "y_add_tens = y_add //10 \r\n",
    "y_add_units = y_add %10 \r\n",
    "y_add_units_cat = to_categorical(y_add_units)\r\n",
    "\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "test_size = 5000\r\n",
    "random_labels1 = np.random.randint(0,10000,test_size)\r\n",
    "random_labels2 = np.random.randint(0,10000,test_size)\r\n",
    "\r\n",
    "x_test_1 = x_test[random_labels1]\r\n",
    "x_test_2 = x_test[random_labels2]\r\n",
    "\r\n",
    "y_test_1 = y_test[random_labels1]\r\n",
    "y_test_2 = y_test[random_labels2]\r\n",
    "\r\n",
    "y_test_add = y_test_1 + y_test_2\r\n",
    "y_test_add_tens = y_test_add //10 \r\n",
    "y_test_add_units = y_test_add %10 \r\n",
    "y_test_add_units_cat = to_categorical(y_test_add_units)\r\n",
    "\r\n",
    "# generate a dataset for multiplication\r\n",
    "\r\n",
    "y_mult = y_train_1 * y_train_2\r\n",
    "y_mult_tens = y_mult //10 \r\n",
    "y_mult_units = y_mult %10 \r\n",
    "y_mult_units_cat = to_categorical(y_mult_units)\r\n",
    "y_mult_tens_cat = to_categorical(y_mult_tens)\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "\r\n",
    "y_test_mult = y_test_1 * y_test_2\r\n",
    "y_test_mult_tens = y_test_mult //10 \r\n",
    "y_test_mult_units = y_test_mult %10 \r\n",
    "y_test_mult_units_cat = to_categorical(y_test_mult_units)\r\n",
    "y_test_mult_tens_cat = to_categorical(y_test_mult_tens)\r\n",
    "\r\n",
    "# generate a dataset for comparison\r\n",
    "y_comp = y_train_1 > y_train_2\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "y_test_comp = y_test_1 > y_test_2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to train our model! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "history = model_complete.fit([x_train_1,x_train_2],[y_add_units_cat,y_add_tens,y_mult_units_cat,y_mult_tens_cat,y_comp], batch_size=1000,epochs=1000,validation_split=0.2, verbose=1,callbacks=[es,es,es,es,es,es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "400/400 [==============================] - 31s 77ms/step - loss: 3.6060 - units_add_loss: 1.2366 - tens_add_loss: 0.3254 - units_mult_loss: 0.9394 - tens_mult_loss: 0.7870 - comp_loss: 0.3175 - units_add_acc: 0.5868 - tens_add_acc: 0.8517 - units_mult_acc: 0.6771 - tens_mult_acc: 0.7189 - comp_acc: 0.8551 - val_loss: 1.7964 - val_units_add_loss: 0.5861 - val_tens_add_loss: 0.1868 - val_units_mult_loss: 0.4516 - val_tens_mult_loss: 0.3929 - val_comp_loss: 0.1790 - val_units_add_acc: 0.8174 - val_tens_add_acc: 0.9246 - val_units_mult_acc: 0.8553 - val_tens_mult_acc: 0.8673 - val_comp_acc: 0.9296\n",
      "Epoch 2/1000\n",
      "400/400 [==============================] - 30s 75ms/step - loss: 1.2846 - units_add_loss: 0.4152 - tens_add_loss: 0.1350 - units_mult_loss: 0.3276 - tens_mult_loss: 0.2757 - comp_loss: 0.1311 - units_add_acc: 0.8730 - tens_add_acc: 0.9490 - units_mult_acc: 0.8956 - tens_mult_acc: 0.9096 - comp_acc: 0.9504 - val_loss: 0.9736 - val_units_add_loss: 0.3122 - val_tens_add_loss: 0.1030 - val_units_mult_loss: 0.2509 - val_tens_mult_loss: 0.2061 - val_comp_loss: 0.1014 - val_units_add_acc: 0.9043 - val_tens_add_acc: 0.9621 - val_units_mult_acc: 0.9212 - val_tens_mult_acc: 0.9330 - val_comp_acc: 0.9626\n",
      "Epoch 3/1000\n",
      "400/400 [==============================] - 30s 75ms/step - loss: 0.7060 - units_add_loss: 0.2232 - tens_add_loss: 0.0762 - units_mult_loss: 0.1806 - tens_mult_loss: 0.1504 - comp_loss: 0.0756 - units_add_acc: 0.9329 - tens_add_acc: 0.9729 - units_mult_acc: 0.9439 - tens_mult_acc: 0.9519 - comp_acc: 0.9728 - val_loss: 0.6057 - val_units_add_loss: 0.1912 - val_tens_add_loss: 0.0649 - val_units_mult_loss: 0.1573 - val_tens_mult_loss: 0.1288 - val_comp_loss: 0.0635 - val_units_add_acc: 0.9408 - val_tens_add_acc: 0.9768 - val_units_mult_acc: 0.9503 - val_tens_mult_acc: 0.9578 - val_comp_acc: 0.9773\n",
      "Epoch 4/1000\n",
      "400/400 [==============================] - 30s 75ms/step - loss: 0.4065 - units_add_loss: 0.1245 - tens_add_loss: 0.0458 - units_mult_loss: 0.1036 - tens_mult_loss: 0.0868 - comp_loss: 0.0458 - units_add_acc: 0.9640 - tens_add_acc: 0.9844 - units_mult_acc: 0.9689 - tens_mult_acc: 0.9735 - comp_acc: 0.9847 - val_loss: 0.3853 - val_units_add_loss: 0.1159 - val_tens_add_loss: 0.0467 - val_units_mult_loss: 0.0968 - val_tens_mult_loss: 0.0845 - val_comp_loss: 0.0413 - val_units_add_acc: 0.9650 - val_tens_add_acc: 0.9833 - val_units_mult_acc: 0.9700 - val_tens_mult_acc: 0.9728 - val_comp_acc: 0.9855\n",
      "Epoch 5/1000\n",
      "400/400 [==============================] - 30s 76ms/step - loss: 0.2258 - units_add_loss: 0.0660 - tens_add_loss: 0.0271 - units_mult_loss: 0.0574 - tens_mult_loss: 0.0482 - comp_loss: 0.0272 - units_add_acc: 0.9829 - tens_add_acc: 0.9916 - units_mult_acc: 0.9843 - tens_mult_acc: 0.9866 - comp_acc: 0.9917 - val_loss: 0.2461 - val_units_add_loss: 0.0713 - val_tens_add_loss: 0.0305 - val_units_mult_loss: 0.0617 - val_tens_mult_loss: 0.0534 - val_comp_loss: 0.0292 - val_units_add_acc: 0.9794 - val_tens_add_acc: 0.9892 - val_units_mult_acc: 0.9809 - val_tens_mult_acc: 0.9831 - val_comp_acc: 0.9899\n",
      "Epoch 6/1000\n",
      "400/400 [==============================] - 30s 75ms/step - loss: 0.1198 - units_add_loss: 0.0328 - tens_add_loss: 0.0155 - units_mult_loss: 0.0301 - tens_mult_loss: 0.0255 - comp_loss: 0.0158 - units_add_acc: 0.9934 - tens_add_acc: 0.9959 - units_mult_acc: 0.9933 - tens_mult_acc: 0.9944 - comp_acc: 0.9958 - val_loss: 0.1763 - val_units_add_loss: 0.0487 - val_tens_add_loss: 0.0224 - val_units_mult_loss: 0.0428 - val_tens_mult_loss: 0.0386 - val_comp_loss: 0.0238 - val_units_add_acc: 0.9854 - val_tens_add_acc: 0.9924 - val_units_mult_acc: 0.9865 - val_tens_mult_acc: 0.9874 - val_comp_acc: 0.9912\n",
      "Epoch 7/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 0.0632 - units_add_loss: 0.0162 - tens_add_loss: 0.0086 - units_mult_loss: 0.0159 - tens_mult_loss: 0.0135 - comp_loss: 0.0091 - units_add_acc: 0.9980 - tens_add_acc: 0.9983 - units_mult_acc: 0.9974 - tens_mult_acc: 0.9979 - comp_acc: 0.9981 - val_loss: 0.1263 - val_units_add_loss: 0.0337 - val_tens_add_loss: 0.0171 - val_units_mult_loss: 0.0314 - val_tens_mult_loss: 0.0277 - val_comp_loss: 0.0164 - val_units_add_acc: 0.9898 - val_tens_add_acc: 0.9941 - val_units_mult_acc: 0.9899 - val_tens_mult_acc: 0.9909 - val_comp_acc: 0.9944\n",
      "Epoch 8/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 0.0337 - units_add_loss: 0.0083 - tens_add_loss: 0.0048 - units_mult_loss: 0.0084 - tens_mult_loss: 0.0072 - comp_loss: 0.0051 - units_add_acc: 0.9996 - tens_add_acc: 0.9994 - units_mult_acc: 0.9992 - tens_mult_acc: 0.9994 - comp_acc: 0.9993 - val_loss: 0.0980 - val_units_add_loss: 0.0252 - val_tens_add_loss: 0.0139 - val_units_mult_loss: 0.0236 - val_tens_mult_loss: 0.0213 - val_comp_loss: 0.0140 - val_units_add_acc: 0.9924 - val_tens_add_acc: 0.9952 - val_units_mult_acc: 0.9925 - val_tens_mult_acc: 0.9930 - val_comp_acc: 0.9952\n",
      "Epoch 9/1000\n",
      "400/400 [==============================] - 29s 74ms/step - loss: 0.0187 - units_add_loss: 0.0045 - tens_add_loss: 0.0027 - units_mult_loss: 0.0046 - tens_mult_loss: 0.0040 - comp_loss: 0.0029 - units_add_acc: 0.9999 - tens_add_acc: 0.9998 - units_mult_acc: 0.9998 - tens_mult_acc: 0.9999 - comp_acc: 0.9998 - val_loss: 0.0817 - val_units_add_loss: 0.0206 - val_tens_add_loss: 0.0121 - val_units_mult_loss: 0.0195 - val_tens_mult_loss: 0.0178 - val_comp_loss: 0.0118 - val_units_add_acc: 0.9938 - val_tens_add_acc: 0.9958 - val_units_mult_acc: 0.9937 - val_tens_mult_acc: 0.9944 - val_comp_acc: 0.9959\n",
      "Epoch 10/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 0.0110 - units_add_loss: 0.0027 - tens_add_loss: 0.0016 - units_mult_loss: 0.0027 - tens_mult_loss: 0.0024 - comp_loss: 0.0017 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0708 - val_units_add_loss: 0.0177 - val_tens_add_loss: 0.0108 - val_units_mult_loss: 0.0169 - val_tens_mult_loss: 0.0154 - val_comp_loss: 0.0100 - val_units_add_acc: 0.9946 - val_tens_add_acc: 0.9963 - val_units_mult_acc: 0.9946 - val_tens_mult_acc: 0.9951 - val_comp_acc: 0.9966\n",
      "Epoch 11/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 0.0069 - units_add_loss: 0.0017 - tens_add_loss: 9.3899e-04 - units_mult_loss: 0.0018 - tens_mult_loss: 0.0015 - comp_loss: 9.7544e-04 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0630 - val_units_add_loss: 0.0154 - val_tens_add_loss: 0.0097 - val_units_mult_loss: 0.0146 - val_tens_mult_loss: 0.0138 - val_comp_loss: 0.0095 - val_units_add_acc: 0.9952 - val_tens_add_acc: 0.9967 - val_units_mult_acc: 0.9952 - val_tens_mult_acc: 0.9954 - val_comp_acc: 0.9967\n",
      "Epoch 12/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 0.0045 - units_add_loss: 0.0011 - tens_add_loss: 5.8934e-04 - units_mult_loss: 0.0012 - tens_mult_loss: 0.0010 - comp_loss: 6.2795e-04 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0581 - val_units_add_loss: 0.0139 - val_tens_add_loss: 0.0094 - val_units_mult_loss: 0.0133 - val_tens_mult_loss: 0.0125 - val_comp_loss: 0.0090 - val_units_add_acc: 0.9956 - val_tens_add_acc: 0.9968 - val_units_mult_acc: 0.9954 - val_tens_mult_acc: 0.9959 - val_comp_acc: 0.9969\n",
      "Epoch 13/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 0.0032 - units_add_loss: 8.1632e-04 - tens_add_loss: 4.0529e-04 - units_mult_loss: 8.0554e-04 - tens_mult_loss: 7.1484e-04 - comp_loss: 4.2707e-04 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0542 - val_units_add_loss: 0.0128 - val_tens_add_loss: 0.0089 - val_units_mult_loss: 0.0123 - val_tens_mult_loss: 0.0118 - val_comp_loss: 0.0085 - val_units_add_acc: 0.9956 - val_tens_add_acc: 0.9970 - val_units_mult_acc: 0.9957 - val_tens_mult_acc: 0.9960 - val_comp_acc: 0.9971\n",
      "Epoch 14/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 0.0023 - units_add_loss: 5.9617e-04 - tens_add_loss: 2.8959e-04 - units_mult_loss: 5.8276e-04 - tens_mult_loss: 5.1624e-04 - comp_loss: 3.0404e-04 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0517 - val_units_add_loss: 0.0119 - val_tens_add_loss: 0.0090 - val_units_mult_loss: 0.0115 - val_tens_mult_loss: 0.0112 - val_comp_loss: 0.0082 - val_units_add_acc: 0.9961 - val_tens_add_acc: 0.9971 - val_units_mult_acc: 0.9961 - val_tens_mult_acc: 0.9962 - val_comp_acc: 0.9973\n",
      "Epoch 15/1000\n",
      "400/400 [==============================] - 29s 74ms/step - loss: 0.0017 - units_add_loss: 4.4356e-04 - tens_add_loss: 2.1200e-04 - units_mult_loss: 4.3015e-04 - tens_mult_loss: 3.7953e-04 - comp_loss: 2.2281e-04 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0497 - val_units_add_loss: 0.0113 - val_tens_add_loss: 0.0087 - val_units_mult_loss: 0.0110 - val_tens_mult_loss: 0.0108 - val_comp_loss: 0.0079 - val_units_add_acc: 0.9961 - val_tens_add_acc: 0.9973 - val_units_mult_acc: 0.9962 - val_tens_mult_acc: 0.9963 - val_comp_acc: 0.9973\n",
      "Epoch 16/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 0.0013 - units_add_loss: 3.3576e-04 - tens_add_loss: 1.5717e-04 - units_mult_loss: 3.2382e-04 - tens_mult_loss: 2.8746e-04 - comp_loss: 1.6480e-04 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0485 - val_units_add_loss: 0.0107 - val_tens_add_loss: 0.0087 - val_units_mult_loss: 0.0106 - val_tens_mult_loss: 0.0105 - val_comp_loss: 0.0080 - val_units_add_acc: 0.9963 - val_tens_add_acc: 0.9972 - val_units_mult_acc: 0.9963 - val_tens_mult_acc: 0.9963 - val_comp_acc: 0.9973\n",
      "Epoch 17/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 9.6360e-04 - units_add_loss: 2.5649e-04 - tens_add_loss: 1.1829e-04 - units_mult_loss: 2.4490e-04 - tens_mult_loss: 2.1826e-04 - comp_loss: 1.2566e-04 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0459 - val_units_add_loss: 0.0101 - val_tens_add_loss: 0.0084 - val_units_mult_loss: 0.0099 - val_tens_mult_loss: 0.0099 - val_comp_loss: 0.0076 - val_units_add_acc: 0.9965 - val_tens_add_acc: 0.9974 - val_units_mult_acc: 0.9966 - val_tens_mult_acc: 0.9966 - val_comp_acc: 0.9975\n",
      "Epoch 18/1000\n",
      "400/400 [==============================] - 29s 74ms/step - loss: 7.3538e-04 - units_add_loss: 1.9739e-04 - tens_add_loss: 9.0139e-05 - units_mult_loss: 1.8691e-04 - tens_mult_loss: 1.6716e-04 - comp_loss: 9.3777e-05 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0448 - val_units_add_loss: 0.0096 - val_tens_add_loss: 0.0083 - val_units_mult_loss: 0.0096 - val_tens_mult_loss: 0.0096 - val_comp_loss: 0.0076 - val_units_add_acc: 0.9968 - val_tens_add_acc: 0.9973 - val_units_mult_acc: 0.9967 - val_tens_mult_acc: 0.9967 - val_comp_acc: 0.9974\n",
      "Epoch 19/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 5.6515e-04 - units_add_loss: 1.5235e-04 - tens_add_loss: 6.9554e-05 - units_mult_loss: 1.4329e-04 - tens_mult_loss: 1.2821e-04 - comp_loss: 7.1747e-05 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0429 - val_units_add_loss: 0.0090 - val_tens_add_loss: 0.0081 - val_units_mult_loss: 0.0092 - val_tens_mult_loss: 0.0093 - val_comp_loss: 0.0073 - val_units_add_acc: 0.9969 - val_tens_add_acc: 0.9975 - val_units_mult_acc: 0.9968 - val_tens_mult_acc: 0.9968 - val_comp_acc: 0.9976\n",
      "Epoch 20/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 4.3602e-04 - units_add_loss: 1.1849e-04 - tens_add_loss: 5.2984e-05 - units_mult_loss: 1.1121e-04 - tens_mult_loss: 9.8957e-05 - comp_loss: 5.4378e-05 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0415 - val_units_add_loss: 0.0085 - val_tens_add_loss: 0.0079 - val_units_mult_loss: 0.0088 - val_tens_mult_loss: 0.0090 - val_comp_loss: 0.0072 - val_units_add_acc: 0.9971 - val_tens_add_acc: 0.9974 - val_units_mult_acc: 0.9969 - val_tens_mult_acc: 0.9969 - val_comp_acc: 0.9977\n",
      "Epoch 21/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 3.3877e-04 - units_add_loss: 9.2501e-05 - tens_add_loss: 4.0540e-05 - units_mult_loss: 8.6084e-05 - tens_mult_loss: 7.7060e-05 - comp_loss: 4.2583e-05 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0407 - val_units_add_loss: 0.0083 - val_tens_add_loss: 0.0080 - val_units_mult_loss: 0.0086 - val_tens_mult_loss: 0.0086 - val_comp_loss: 0.0072 - val_units_add_acc: 0.9972 - val_tens_add_acc: 0.9976 - val_units_mult_acc: 0.9971 - val_tens_mult_acc: 0.9971 - val_comp_acc: 0.9976\n",
      "Epoch 22/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 2.6322e-04 - units_add_loss: 7.2051e-05 - tens_add_loss: 3.1578e-05 - units_mult_loss: 6.6763e-05 - tens_mult_loss: 5.9971e-05 - comp_loss: 3.2853e-05 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0398 - val_units_add_loss: 0.0080 - val_tens_add_loss: 0.0080 - val_units_mult_loss: 0.0082 - val_tens_mult_loss: 0.0085 - val_comp_loss: 0.0071 - val_units_add_acc: 0.9973 - val_tens_add_acc: 0.9976 - val_units_mult_acc: 0.9971 - val_tens_mult_acc: 0.9971 - val_comp_acc: 0.9977\n",
      "Epoch 23/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 2.0571e-04 - units_add_loss: 5.6573e-05 - tens_add_loss: 2.4426e-05 - units_mult_loss: 5.2218e-05 - tens_mult_loss: 4.6886e-05 - comp_loss: 2.5604e-05 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0391 - val_units_add_loss: 0.0077 - val_tens_add_loss: 0.0079 - val_units_mult_loss: 0.0081 - val_tens_mult_loss: 0.0083 - val_comp_loss: 0.0071 - val_units_add_acc: 0.9974 - val_tens_add_acc: 0.9976 - val_units_mult_acc: 0.9972 - val_tens_mult_acc: 0.9972 - val_comp_acc: 0.9977\n",
      "Epoch 24/1000\n",
      "400/400 [==============================] - 29s 74ms/step - loss: 1.6170e-04 - units_add_loss: 4.4446e-05 - tens_add_loss: 1.9269e-05 - units_mult_loss: 4.0984e-05 - tens_mult_loss: 3.6805e-05 - comp_loss: 2.0197e-05 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0385 - val_units_add_loss: 0.0075 - val_tens_add_loss: 0.0080 - val_units_mult_loss: 0.0078 - val_tens_mult_loss: 0.0082 - val_comp_loss: 0.0071 - val_units_add_acc: 0.9974 - val_tens_add_acc: 0.9977 - val_units_mult_acc: 0.9973 - val_tens_mult_acc: 0.9973 - val_comp_acc: 0.9977\n",
      "Epoch 25/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 1.2717e-04 - units_add_loss: 3.5151e-05 - tens_add_loss: 1.5155e-05 - units_mult_loss: 3.2250e-05 - tens_mult_loss: 2.8863e-05 - comp_loss: 1.5753e-05 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0376 - val_units_add_loss: 0.0072 - val_tens_add_loss: 0.0078 - val_units_mult_loss: 0.0077 - val_tens_mult_loss: 0.0080 - val_comp_loss: 0.0069 - val_units_add_acc: 0.9976 - val_tens_add_acc: 0.9977 - val_units_mult_acc: 0.9973 - val_tens_mult_acc: 0.9973 - val_comp_acc: 0.9977\n",
      "Epoch 26/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 9.9676e-05 - units_add_loss: 2.7638e-05 - tens_add_loss: 1.1989e-05 - units_mult_loss: 2.5163e-05 - tens_mult_loss: 2.2756e-05 - comp_loss: 1.2130e-05 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0368 - val_units_add_loss: 0.0069 - val_tens_add_loss: 0.0078 - val_units_mult_loss: 0.0074 - val_tens_mult_loss: 0.0078 - val_comp_loss: 0.0069 - val_units_add_acc: 0.9976 - val_tens_add_acc: 0.9978 - val_units_mult_acc: 0.9974 - val_tens_mult_acc: 0.9974 - val_comp_acc: 0.9978\n",
      "Epoch 27/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 7.8589e-05 - units_add_loss: 2.1887e-05 - tens_add_loss: 9.2781e-06 - units_mult_loss: 1.9864e-05 - tens_mult_loss: 1.7899e-05 - comp_loss: 9.6607e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0362 - val_units_add_loss: 0.0067 - val_tens_add_loss: 0.0077 - val_units_mult_loss: 0.0073 - val_tens_mult_loss: 0.0076 - val_comp_loss: 0.0069 - val_units_add_acc: 0.9977 - val_tens_add_acc: 0.9977 - val_units_mult_acc: 0.9974 - val_tens_mult_acc: 0.9974 - val_comp_acc: 0.9978\n",
      "Epoch 28/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 6.1932e-05 - units_add_loss: 1.7338e-05 - tens_add_loss: 7.3008e-06 - units_mult_loss: 1.5650e-05 - tens_mult_loss: 1.4115e-05 - comp_loss: 7.5283e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0357 - val_units_add_loss: 0.0065 - val_tens_add_loss: 0.0078 - val_units_mult_loss: 0.0071 - val_tens_mult_loss: 0.0075 - val_comp_loss: 0.0069 - val_units_add_acc: 0.9978 - val_tens_add_acc: 0.9978 - val_units_mult_acc: 0.9975 - val_tens_mult_acc: 0.9975 - val_comp_acc: 0.9978\n",
      "Epoch 29/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 4.8954e-05 - units_add_loss: 1.3667e-05 - tens_add_loss: 5.8006e-06 - units_mult_loss: 1.2396e-05 - tens_mult_loss: 1.1174e-05 - comp_loss: 5.9157e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0350 - val_units_add_loss: 0.0063 - val_tens_add_loss: 0.0078 - val_units_mult_loss: 0.0068 - val_tens_mult_loss: 0.0073 - val_comp_loss: 0.0068 - val_units_add_acc: 0.9978 - val_tens_add_acc: 0.9978 - val_units_mult_acc: 0.9976 - val_tens_mult_acc: 0.9976 - val_comp_acc: 0.9978\n",
      "Epoch 30/1000\n",
      "400/400 [==============================] - 30s 75ms/step - loss: 3.8788e-05 - units_add_loss: 1.0893e-05 - tens_add_loss: 4.5628e-06 - units_mult_loss: 9.7750e-06 - tens_mult_loss: 8.8761e-06 - comp_loss: 4.6818e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0348 - val_units_add_loss: 0.0061 - val_tens_add_loss: 0.0078 - val_units_mult_loss: 0.0068 - val_tens_mult_loss: 0.0074 - val_comp_loss: 0.0068 - val_units_add_acc: 0.9979 - val_tens_add_acc: 0.9978 - val_units_mult_acc: 0.9977 - val_tens_mult_acc: 0.9976 - val_comp_acc: 0.9978\n",
      "Epoch 31/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 3.0736e-05 - units_add_loss: 8.6567e-06 - tens_add_loss: 3.6184e-06 - units_mult_loss: 7.7521e-06 - tens_mult_loss: 6.9844e-06 - comp_loss: 3.7247e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0343 - val_units_add_loss: 0.0060 - val_tens_add_loss: 0.0077 - val_units_mult_loss: 0.0066 - val_tens_mult_loss: 0.0071 - val_comp_loss: 0.0069 - val_units_add_acc: 0.9979 - val_tens_add_acc: 0.9979 - val_units_mult_acc: 0.9978 - val_tens_mult_acc: 0.9977 - val_comp_acc: 0.9979\n",
      "Epoch 32/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 2.4450e-05 - units_add_loss: 6.8998e-06 - tens_add_loss: 2.8827e-06 - units_mult_loss: 6.1598e-06 - tens_mult_loss: 5.5641e-06 - comp_loss: 2.9438e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0336 - val_units_add_loss: 0.0058 - val_tens_add_loss: 0.0077 - val_units_mult_loss: 0.0064 - val_tens_mult_loss: 0.0070 - val_comp_loss: 0.0067 - val_units_add_acc: 0.9980 - val_tens_add_acc: 0.9978 - val_units_mult_acc: 0.9977 - val_tens_mult_acc: 0.9977 - val_comp_acc: 0.9980\n",
      "Epoch 33/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 1.9478e-05 - units_add_loss: 5.5148e-06 - tens_add_loss: 2.2782e-06 - units_mult_loss: 4.8993e-06 - tens_mult_loss: 4.4402e-06 - comp_loss: 2.3451e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0336 - val_units_add_loss: 0.0058 - val_tens_add_loss: 0.0077 - val_units_mult_loss: 0.0063 - val_tens_mult_loss: 0.0071 - val_comp_loss: 0.0067 - val_units_add_acc: 0.9980 - val_tens_add_acc: 0.9979 - val_units_mult_acc: 0.9978 - val_tens_mult_acc: 0.9978 - val_comp_acc: 0.9979\n",
      "Epoch 34/1000\n",
      "400/400 [==============================] - 29s 74ms/step - loss: 1.5500e-05 - units_add_loss: 4.3918e-06 - tens_add_loss: 1.8143e-06 - units_mult_loss: 3.8989e-06 - tens_mult_loss: 3.5252e-06 - comp_loss: 1.8695e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0333 - val_units_add_loss: 0.0056 - val_tens_add_loss: 0.0079 - val_units_mult_loss: 0.0061 - val_tens_mult_loss: 0.0068 - val_comp_loss: 0.0068 - val_units_add_acc: 0.9982 - val_tens_add_acc: 0.9979 - val_units_mult_acc: 0.9979 - val_tens_mult_acc: 0.9978 - val_comp_acc: 0.9980\n",
      "Epoch 35/1000\n",
      "400/400 [==============================] - 29s 74ms/step - loss: 1.2360e-05 - units_add_loss: 3.4987e-06 - tens_add_loss: 1.4512e-06 - units_mult_loss: 3.1188e-06 - tens_mult_loss: 2.8161e-06 - comp_loss: 1.4752e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0327 - val_units_add_loss: 0.0054 - val_tens_add_loss: 0.0078 - val_units_mult_loss: 0.0060 - val_tens_mult_loss: 0.0068 - val_comp_loss: 0.0067 - val_units_add_acc: 0.9982 - val_tens_add_acc: 0.9979 - val_units_mult_acc: 0.9979 - val_tens_mult_acc: 0.9979 - val_comp_acc: 0.9979\n",
      "Epoch 36/1000\n",
      "400/400 [==============================] - ETA: 0s - loss: 9.8763e-06 - units_add_loss: 2.8118e-06 - tens_add_loss: 1.1577e-06 - units_mult_loss: 2.4849e-06 - tens_mult_loss: 2.2442e-06 - comp_loss: 1.1778e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000Restoring model weights from the end of the best epoch.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 9.8763e-06 - units_add_loss: 2.8118e-06 - tens_add_loss: 1.1577e-06 - units_mult_loss: 2.4849e-06 - tens_mult_loss: 2.2442e-06 - comp_loss: 1.1778e-06 - units_add_acc: 1.0000 - tens_add_acc: 1.0000 - units_mult_acc: 1.0000 - tens_mult_acc: 1.0000 - comp_acc: 1.0000 - val_loss: 0.0328 - val_units_add_loss: 0.0053 - val_tens_add_loss: 0.0077 - val_units_mult_loss: 0.0060 - val_tens_mult_loss: 0.0069 - val_comp_loss: 0.0068 - val_units_add_acc: 0.9983 - val_tens_add_acc: 0.9980 - val_units_mult_acc: 0.9979 - val_tens_mult_acc: 0.9978 - val_comp_acc: 0.9979\n",
      "Epoch 00036: early stopping\n",
      "Epoch 00036: early stopping\n",
      "Epoch 00036: early stopping\n",
      "Epoch 00036: early stopping\n",
      "Epoch 00036: early stopping\n",
      "Epoch 00036: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of the training, the accuracy on all outputs is pretty good (9x%). Let's see first how the model performs on the test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "test = model_complete.evaluate([x_test_1,x_test_2],[y_test_add_units_cat,y_test_add_tens,y_test_mult_units_cat,y_test_mult_tens_cat,y_test_comp], batch_size=100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50/50 [==============================] - 1s 11ms/step - loss: 2.4764 - units_add_loss: 0.7290 - tens_add_loss: 0.2871 - units_mult_loss: 0.6328 - tens_mult_loss: 0.5221 - comp_loss: 0.3055 - units_add_acc: 0.9140 - tens_add_acc: 0.9646 - units_mult_acc: 0.9292 - tens_mult_acc: 0.9416 - comp_acc: 0.9640\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results are still in the 9x%. We can show a random sample of the model predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "random_label_1 = np.random.randint(0,9999)\r\n",
    "random_label_2 = np.random.randint(0,9999)\r\n",
    "\r\n",
    "img_sample1 = x_test[random_label_1,:,:].reshape((1,28,28,1))\r\n",
    "img_sample2 = x_test[random_label_2,:,:].reshape((1,28,28,1))\r\n",
    "\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.imshow(img_sample1.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.imshow(img_sample2.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "prediction = model_complete.predict([img_sample1,img_sample2])\r\n",
    "unit_add = prediction[0]\r\n",
    "ten_add = prediction[1]\r\n",
    "unit_mult = prediction[2]\r\n",
    "ten_mult = prediction[3]\r\n",
    "comp_images = prediction[4]\r\n",
    "\r\n",
    "sum_images = np.argmax(unit_add)+10*np.round(ten_add)\r\n",
    "print('sum =',sum_images)\r\n",
    "\r\n",
    "\r\n",
    "mult_images = np.argmax(unit_mult)+10*np.argmax(ten_mult)\r\n",
    "print('multiplication result =',mult_images)\r\n",
    "\r\n",
    "print('comparison result =',np.round(comp_images),'1 if the number on the left is greater, 0 elsewhere')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sum = [[12.]]\n",
      "multiplication result = 32\n",
      "comparison result = [[0.]] 1 if the number on the left is greater, 0 elsewhere\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAC4CAYAAAD61bdSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQZUlEQVR4nO3deYyVZZbH8d8RNcDg0thhicvQKjEBXDBEGgcN6mhr024JYXFHHUxsiE4MCTGuoybG2K2TOFGrAwECgo2sMdBjCx1pE0QWSSM6ThMCKoECFRENpEHP/FGXSenzXOrW3arOre8nMdz7q+fe93mp48nLu5q7CwAQz3EdPQEAQHlo4AAQFA0cAIKigQNAUDRwAAiKBg4AQVXUwM3sWjP7xMy2mtm0ak0K6GjUNiKwcs8DN7Nukv5X0tWSPpe0TtIEd//oGJ/hpHPUlLtbpd9BbaMzytV2JVvgl0ja6u7b3P0fkuZLurGC7wM6C2obIVTSwE+X9Fmr958Xsh8xs0lmtt7M1lewLKCeqG2EcHytF+DuTZKaJP6ZicZCbaOjVbIFvlPSma3en1HIgOiobYRQSQNfJ2mgmf3CzE6UNF7SsupMC+hQ1DZCKHsXirsfMbPJkv5bUjdJM9x9S9VmBnQQahtRlH0aYVkLYz8haqwapxGWg9pGrVX7NEIAQAeigQNAUDRwAAiKBg4AQdHAASAoGjgABEUDB4CgaOAAEBQNHACCooEDQFA0cAAIigYOAEHRwAEgKBo4AARFAweAoGjgABAUDRwAgqKBA0BQZT8TU5LMbLukA5K+l3TE3YdVY1KonsmTJyfZo48+mh171113JdmKFSuqPaUQqG1EUFEDL7jC3b+owvcAnQ21jU6NXSgAEFSlDdwlvWVmG8xsUjUmBHQS1DY6vUp3oYx0951m1kfSn83sf9x9desBheLnfwBEQ22j06toC9zddxb+3CNpsaRLMmOa3H0YB4EQCbWNCMreAjezf5J0nLsfKLy+RtJ/VG1mncC5556bZJdeeml27OzZs2s9nWNavHhxNj///POTrE+fPtmxt99+e5J1xbNQukJtt8eUKVOS7KqrrsqO7dWrV5INGjQoyfr165f9/M6dO5Ps3XffzY7dsGFDki1ZsiQ7duvWrdk8ukp2ofSVtNjMjn7Pa+7+p6rMCuhY1DZCKLuBu/s2SRdWcS5Ap0BtIwpOIwSAoGjgABBUNa7EbFjTp09PsqFDh2bHbt++PclWr16dDqyRnj17ZvNzzjmn5O/YvHlztaaDgJYvX57Nr7nmmiT79NNPs2M/+eSTJMsdWGxubs5+fsSIEUmWO5lAksaNG5dkzzzzTHbstGnTkuyFF17Ijo2ELXAACIoGDgBB0cABICgaOAAERQMHgKA4C+UYcmd2nHTSSdmxb775ZpL17t07O/bIkSMVzatHjx5J1p6zTYpZt25dxd+BuAYOHJjNjzsu3c5buHBhduzUqVOrOidJOuGEE7L52WefnWRjxozJjn3yySeTbO/evdmxc+bMacfsOhZb4AAQFA0cAIKigQNAUDRwAAjK3L1+CzOr38Kq4KmnnkqyRx55pOTPFzv4UulBzAkTJiTZa6+9VvLnP/vss2w+ePDgJDtw4EDpE+sE3N06YrnRajvnlVdeyeaTJqUPHbr88suzY4vdu7ujrV27NslyJwNI0gUXXFDr6ZQlV9tsgQNAUDRwAAiKBg4AQdHAASAoGjgABNXmpfRmNkPSbyTtcfchhay3pNclDZC0XdJYd99Xu2l2jP3795c8dtWqVUn2/fffV3M6/+/UU0+t6PPFnjQf7YyTSnXl2s55/vnns/kdd9yRZN27d6/1dKrqrbfeSrLcAyGiKWULfKaka3+STZO00t0HSlpZeA9EM1PUNgJrs4G7+2pJX/0kvlHSrMLrWZJuqu60gNqjthFduXcj7Ovuuwqvd0vqW2ygmU2SlF4JAHRO1DbCqPh2su7ux7oKzd2bJDVJjXG1GroOahudXbkNvNnM+rv7LjPrL2lPNSfVWbTnIMfSpUuTrFa3KVizZk1Fn3/vvfeqNJOG1CVqO2fr1q3ZfNOmTUn2q1/9Kjv27bffruaUqiZ3m4hGUO5phMsk3Vl4faektHsBMVHbCKPNBm5m8yStkXSemX1uZvdIelbS1Wb2d0n/WngPhEJtI7o2d6G4e3rruxZXVXkuQF1R24iOKzEBICgaOAAExVPp2+nw4cPZPHcWSq2MHTu2bssCli9fnmQTJ07Mjn311VeTrNjZLbXQr1+/bH7ZZZcl2fz582s9nZpjCxwAgqKBA0BQNHAACIoGDgBBcRCznYo9aX7IkCFJtmPHjprMYe/evTX5XiAndy/txx57LDv2gQceSLIpU6ZUfU7FnHHGGdk8dw/9BQsW1Hg2tccWOAAERQMHgKBo4AAQFA0cAILiIGaVFDu4WQvFrgYtVaUPRUbX8v777yfZiy++mB2bO5jfrVu3JKvGA79zV10+99xz2bEffvhhkq1evbriOXQ0tsABICgaOAAERQMHgKBo4AAQFA0cAIJq8ywUM5sh6TeS9rj7kEL2hKR/k3T0mu6H3T29aTBqYvTo0SWPPXjwYJK9/PLL1ZxOWNR2+WbPnp3NV65cmWSPP/54khW7FL897rnnniQbNWpUdmyxPLpStsBnSro2k7/g7hcV/qPAEdFMUdsIrM0G7u6rJX1Vh7kAdUVtI7pK9oFPNrO/mdkMM/tZsUFmNsnM1pvZ+gqWBdQTtY0Qym3gL0s6R9JFknZJ+l2xge7e5O7D3H1YmcsC6onaRhhlXUrv7s1HX5vZHyS9WbUZdSKHDh1KsmKXAFd6j+7jj09/Fffee2927MiRI0v+3mXLliVZbr3QoqvUdqVyl6ZL0tNPP51kufuBFzuQvmvXriS78MILs2NzB0KXLFmSHdsIl83nlLUFbmb9W729WVL+twkEQ20jklJOI5wnaZSkn5vZ55IelzTKzC6S5JK2S7qvdlMEaoPaRnRtNnB3n5CJp9dgLkBdUduIjisxASAoGjgABMUDHY7hjTfeSLJiZ4AsXLgwyc4666zs2NNOOy3JZs2alWRXX311W1Ns08aNGyv+DqBUL730UpL16dMnyYo9eOGhhx5Ksjlz5mTHLl26NMluueWWtqbYUNgCB4CgaOAAEBQNHACCooEDQFDm7vVbmFn9FlYFuae3r1q1Kjt26NChSdbc3JwZKfXu3TvJck+1X7x4cfbz+/btS7K77747OzZ3afHNN9+cHdsI3N06YrnRarueJk6cmGTFLqU/fPhwkq1fn79X2A033JBkBw4caOfs4sjVNlvgABAUDRwAgqKBA0BQNHAACIoGDgBBcSn9MXz99ddJdtttt2XHvv7660k2ZMiQkpe1YcOGJJs0aVJ27Pjx40v+3t27d5c8Fl1bz549s/nw4cOTbPTo0SV/7+DBg5PsxBNPzI7dtGlTkl1//fXZsd9++23Jc2hUbIEDQFA0cAAIigYOAEHRwAEgqDYvpTezMyXNltRXLc8JbHL3/zSz3pJelzRALc8OHOvu6TXeP/6uhr3cuHv37kl28sknZ8cOGDAgybZs2ZJk3333Xfbzucv5r7jiiuzYsWPHJtmCBQuyYxtBey6lp7Z/bO7cudl8woT0yXNffPFFdmzuoPm2bduS7ODBg9nP5w7Q9+jRIzv20KFD2bxRlXsp/RFJD7n7IEm/lPRbMxskaZqkle4+UNLKwnsgEmobobXZwN19l7tvLLw+IOljSadLulHS0cfIzJJ0U43mCNQEtY3o2nUeuJkNkDRU0lpJfd19V+FHu9Xyz9DcZyZJyp/QDHQS1DYiKvkgppn1krRQ0oPu/k3rn3nLjvTsPkB3b3L3Ye4+rKKZAjVCbSOqkhq4mZ2glgKf6+6LCnGzmfUv/Ly/pD21mSJQO9Q2ImtzF4qZmaTpkj5299+3+tEySXdKerbwZ/qI6C4kd0S82FHyPXsq6we5M17QftT2j/Xq1Sub585OmTp1anZsqbduOOWUU7L5uHHjkuy6667Lji32wJOupJR94P8i6XZJm81sUyF7WC3F/Uczu0fSDknp+WpA50ZtI7Q2G7i7vyup2Lm1V1V3OkD9UNuIjisxASAoGjgABMX9wANasWJFko0YMaIDZoJGMm/evGze1NSUZIsWLcqMLP3A4v79+7P50qXp8eIxY8ZUtKxGxhY4AARFAweAoGjgABAUDRwAgqKBA0BQnIUS0L59x3y2AFCW+fPnZ/OLL744yZ599tns2A8++CDJtm/fXvIcZs2alWT33XdfyZ/vatgCB4CgaOAAEBQNHACCooEDQFAcxAxox44dHT0FdCFPPPFEkn355ZfZscuXL0+y+++/P8nWrl2b/fyVV16ZZO+8804bM+y62AIHgKBo4AAQFA0cAIKigQNAUG02cDM708z+YmYfmdkWM3ugkD9hZjvNbFPhv1/XfrpA9VDbiM7c/dgDzPpL6u/uG83sJEkbJN2klge9fuvuz5e8MLNjLwyokLsXe8ZlgtquvvPOOy/JZs6cmWTDhw/Pfn7NmjVJduutt2bHtucS/UaQq+1SHmq8S9KuwusDZvaxpNOrPz2gvqhtRNeufeBmNkDSUElHT+KcbGZ/M7MZZvazIp+ZZGbrzWx9ZVMFaofaRkQlN3Az6yVpoaQH3f0bSS9LOkfSRWrZivld7nPu3uTuw9x9WOXTBaqP2kZUJTVwMztBLQU+190XSZK7N7v79+7+g6Q/SLqkdtMEaoPaRmSlHMQ0SbMkfeXuD7bK+xf2IcrM/l3ScHcf38Z3caAHNdXOg5jUNsLI1XYpDXykpL9K2izph0L8sKQJavknpkvaLum+o0V/jO+iyFFT7Wzg1DbCKKuBVxNFjlprTwOvJmobtZarba7EBICgaOAAEBQNHACCooEDQFA0cAAIigYOAEHRwAEgKBo4AARV76fSfyHp6CPVf15432hYr47zzx247KO1HeHvqVyNum4R1itb23W9EvNHCzZb34h3cWO9urZG/ntq1HWLvF7sQgGAoGjgABBURzbwpg5cdi2xXl1bI/89Neq6hV2vDtsHDgCoDLtQACAoGjgABFX3Bm5m15rZJ2a21cym1Xv51VR4YvkeM/uwVdbbzP5sZn8v/Jl9onlnZmZnmtlfzOwjM9tiZg8U8vDrVkuNUtvUdZx1q2sDN7Nukv5L0nWSBkmaYGaD6jmHKpsp6dqfZNMkrXT3gZJWFt5Hc0TSQ+4+SNIvJf228HtqhHWriQar7ZmirkOo9xb4JZK2uvs2d/+HpPmSbqzzHKrG3VdL+uon8Y1qeVCuCn/eVM85VYO773L3jYXXByR9LOl0NcC61VDD1DZ1HWfd6t3AT5f0Wav3nxeyRtK31QNwd0vq25GTqZSZDZA0VNJaNdi6VVmj13ZD/e4bpa45iFlD3nKOZtjzNM2sl6SFkh50929a/yz6uqF80X/3jVTX9W7gOyWd2er9GYWskTSbWX9JKvy5p4PnUxYzO0EtRT7X3RcV4oZYtxpp9NpuiN99o9V1vRv4OkkDzewXZnaipPGSltV5DrW2TNKdhdd3SlragXMpi5mZpOmSPnb337f6Ufh1q6FGr+3wv/tGrOu6X4lpZr+W9KKkbpJmuPszdZ1AFZnZPEmj1HI7ymZJj0taIumPks5Sy+1Fx7r7Tw8IdWpmNlLSXyVtlvRDIX5YLfsLQ69bLTVKbVPXcdaNS+kBICgOYgJAUDRwAAiKBg4AQdHAASAoGjgABEUDB4CgaOAAENT/AdgvXjg+RVxcAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results look promising! We actually could improve the accuracy by training the model on more random samples (increase `train_size` value) or tweak the model architecture. One last thing: save the model!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# save the model\r\n",
    "model_complete.save('model_complete')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion and future work"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "684b1123683431d89d3bfe9a89cc763215f4b8cd94b4aba1fb40ad45ff7c8b41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}