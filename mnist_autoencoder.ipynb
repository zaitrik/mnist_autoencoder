{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This is how a neural network learns to add, multiply and compare handwritten digits WITHOUT knowing their values "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\"> <img src=\"https://i.dlpng.com/static/png/6906777_preview.png\"> </p>   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I described in a [previous post](https://blog.jovian.ai/how-to-train-supervised-machine-learning-algorithms-without-labeled-data-6ebddc01a00f), how useful are autoencoders in  automated labeling. The main property of these networks is their ability to learn features/patterns in the data. This is in fact not specific to autoencoders and can be implemented using other unsupervised techniques, mainly **PCA**.  \r\n",
    "The ability to detect and learn features in data can be used in other areas.  \r\n",
    "\r\n",
    "In this post, I will present some applications of convolutional autoencoders:  \r\n",
    "- First, a convolutional autoencoder will be trained on **MNIST** data.\r\n",
    "- After the training of the encoder and decoder, we will freeze their weights and use them with additional dense layers to \"learn\" arithmetic operations, namely addition, multiplication and comparison.  \r\n",
    "The trick is to *never* explicitly associate the handwritten digits in **MNIST** dataset with their respective labels. We will see that the neural networks will be nevertheless able to reach 97+% accuracy in all cases on unseen data.\r\n",
    "\r\n",
    "The first step is described in the following diagram:\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/chLUEdp.png\"> </p>   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the second step, we will use the encoder in series with dense layers to perform arithmetic operations: addition, multiplication and comparison. We will train only the dense layer weights, and supply the results of the operations as labels. note that we will not supply the digits values (labels).\r\n",
    "\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/s8U8up4.png\"> </p> \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training an autoencoder on MNIST data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar to the previous article, we will use MNIST data in this experiment. The autoencoder will learn the handwritten digits features using 60000 training samples. We import MNIST using *KERAS* library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#import libraries and setup \r\n",
    "import keras\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import logging\r\n",
    "logging.getLogger('tensorflow').disabled = True\r\n",
    "from keras.models import Sequential, Model\r\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, UpSampling2D, Reshape, Concatenate, Input\r\n",
    "from keras.callbacks import EarlyStopping\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, restore_best_weights=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import mnist\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n",
    "print(x_train.shape,y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We scale the data in the range `[0,1]` and reshape it to *KERAS* format for pictures (nbr_samples x width x height x channels) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#normalize data\r\n",
    "if x_train.max() >1:\r\n",
    "    x_train = x_train / 255\r\n",
    "    x_test = x_test / 255\r\n",
    "\r\n",
    "default_shape = x_train.shape\r\n",
    "#reshape input data to 1 channel\r\n",
    "x_train = x_train.reshape(-1,default_shape[1],default_shape[2],1)\r\n",
    "x_test = x_test.reshape(-1,default_shape[1],default_shape[2],1)\r\n",
    "image_dim = x_train.shape[1:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will implement a similar autoencoder architecture as in [[1]](https://blog.jovian.ai/how-to-train-supervised-machine-learning-algorithms-without-labeled-data-6ebddc01a00f). It is based on a series of convolutional layers, that will gradually encode the 28x28 image (784 pixel) into a 100 elements array, and decode that representation back to the original format. The resulting image -after the training step- will hopefully resemble to the original one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# create an autoencoder / decoder \r\n",
    "encoder = Sequential()\r\n",
    "encoder.add(Conv2D(32,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu',input_shape=image_dim))\r\n",
    "encoder.add(MaxPooling2D(2,2))\r\n",
    "encoder.add(Conv2D(64,kernel_size=(3,3), strides=(1,1),padding='same',activation='selu'))\r\n",
    "encoder.add(MaxPooling2D(2,2))\r\n",
    "encoder.add(Conv2D(128,kernel_size=(3,3), strides=(1,1),padding='same',activation='selu'))\r\n",
    "encoder.add(Flatten())\r\n",
    "encoder.add(Dense(100,activation='sigmoid'))\r\n",
    "encoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               627300    \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 719,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "encoder_out_dim = encoder.layers[-1].output_shape[1:] # dimension of the encoder output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "decoder = Sequential()\r\n",
    "decoder.add(Dense(6272, activation='sigmoid', input_shape=encoder_out_dim))\r\n",
    "decoder.add(Reshape(( 7, 7, 128)))\r\n",
    "decoder.add(Conv2D(128,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu'))\r\n",
    "decoder.add(UpSampling2D((2,2)))\r\n",
    "decoder.add(Conv2D(64,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu'))\r\n",
    "decoder.add(UpSampling2D((2,2)))\r\n",
    "decoder.add(Conv2D(1,kernel_size=(3,3), strides=(1,1),padding='same', activation='sigmoid'))\r\n",
    "\r\n",
    "decoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "=================================================================\n",
      "Total params: 855,425\n",
      "Trainable params: 855,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The autoencoder is created using the encoder and the decoder:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "enc_dec = Sequential([encoder,decoder])\r\n",
    "enc_dec.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 100)               719972    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 28, 28, 1)         855425    \n",
      "=================================================================\n",
      "Total params: 1,575,397\n",
      "Trainable params: 1,575,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It will be trained as a set of binary classifiers for each pixel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "enc_dec.compile(optimizer='nadam', loss = 'binary_crossentropy')\r\n",
    "history = enc_dec.fit(x_train,x_train, batch_size=100,epochs=1000,validation_split=0.2, verbose=2,callbacks=[es,es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "480/480 - 14s - loss: 0.2756 - val_loss: 0.2126\n",
      "Epoch 2/1000\n",
      "480/480 - 14s - loss: 0.1451 - val_loss: 0.1127\n",
      "Epoch 3/1000\n",
      "480/480 - 14s - loss: 0.1053 - val_loss: 0.0915\n",
      "Epoch 4/1000\n",
      "480/480 - 14s - loss: 0.0881 - val_loss: 0.0849\n",
      "Epoch 5/1000\n",
      "480/480 - 14s - loss: 0.0805 - val_loss: 0.0781\n",
      "Epoch 6/1000\n",
      "480/480 - 14s - loss: 0.0771 - val_loss: 0.0765\n",
      "Epoch 7/1000\n",
      "480/480 - 14s - loss: 0.0749 - val_loss: 0.0744\n",
      "Epoch 8/1000\n",
      "480/480 - 14s - loss: 0.0734 - val_loss: 0.0736\n",
      "Epoch 9/1000\n",
      "480/480 - 14s - loss: 0.0724 - val_loss: 0.0735\n",
      "Epoch 10/1000\n",
      "480/480 - 14s - loss: 0.0715 - val_loss: 0.0724\n",
      "Epoch 11/1000\n",
      "480/480 - 14s - loss: 0.0707 - val_loss: 0.0717\n",
      "Epoch 12/1000\n",
      "480/480 - 14s - loss: 0.3967 - val_loss: 0.2242\n",
      "Epoch 13/1000\n",
      "480/480 - 14s - loss: 0.2060 - val_loss: 0.1908\n",
      "Epoch 14/1000\n",
      "480/480 - 14s - loss: 0.1805 - val_loss: 0.1718\n",
      "Epoch 15/1000\n",
      "480/480 - 14s - loss: 0.1670 - val_loss: 0.1583\n",
      "Epoch 16/1000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "480/480 - 14s - loss: 0.1597 - val_loss: 0.1598\n",
      "Epoch 00016: early stopping\n",
      "Epoch 00016: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The early stopping will make sure the autoencoder will not overfit the training data. There are two ways to verify the network. First, we can evaluate the loss function on test data, and expect it to be close to the loss value on the training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "enc_dec.evaluate(x_test,x_test,batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10/10 [==============================] - 1s 69ms/step - loss: 0.0706\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.07063404470682144"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "enc_dec.evaluate(x_train,x_train,batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60/60 [==============================] - 4s 75ms/step - loss: 0.0709\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0709402784705162"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is very close, around `0.08` for both data sets. The second method is to check the resulting reconstitution that we obtain for a random sample from the test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "random_label = np.random.randint(0,9999)\r\n",
    "img_sample = x_test[random_label,:,:].reshape((1,28,28,1))\r\n",
    "plt.imshow(img_sample.reshape(28,28), cmap='gray');\r\n",
    "pred_img = enc_dec.predict(img_sample) \r\n",
    "plt.figure();\r\n",
    "plt.imshow(pred_img.reshape(28,28), cmap='gray');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*A picture is worth a thousand words!*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# save models\r\n",
    "encoder.save('encoder')\r\n",
    "decoder.save('decoder')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a trained encoder and decoder, let's focus on the *encoder*. For each image, the encoder generates a representation that captures most of the interesting features. This representation is sufficient to reconstitute the image using the decoder. Here is the representation of the sample image we used earlier: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "representation_sample = encoder.predict(img_sample)\r\n",
    "print(representation_sample) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.6849651  0.34472713 0.25619155 0.23160648 0.35074276 0.4103539\n",
      "  0.55013055 0.42363262 0.7040259  0.23800915 0.53551245 0.24630167\n",
      "  0.2833512  0.69841975 0.3863604  0.43630293 0.28270313 0.5658907\n",
      "  0.49058506 0.3138075  0.29558608 0.14503041 0.75979453 0.30793226\n",
      "  0.6165079  0.8603799  0.11150015 0.29174513 0.20776507 0.6789288\n",
      "  0.4460373  0.49379033 0.3403997  0.36270043 0.57579    0.04207095\n",
      "  0.3040763  0.23057008 0.29035312 0.25128216 0.6152144  0.32471436\n",
      "  0.38769412 0.5180744  0.24011672 0.4718439  0.36379343 0.27261958\n",
      "  0.28814137 0.6392059  0.17767033 0.71380717 0.4549716  0.5914341\n",
      "  0.23667179 0.7512161  0.12477065 0.8605879  0.45556203 0.56907356\n",
      "  0.18660522 0.8400206  0.23126473 0.46350786 0.4202994  0.82483053\n",
      "  0.3710613  0.641825   0.6676602  0.6502603  0.43319064 0.3773252\n",
      "  0.5582269  0.29559913 0.39784604 0.35061973 0.2739762  0.83289695\n",
      "  0.27447444 0.8513281  0.31333598 0.8286474  0.23605205 0.66181344\n",
      "  0.41684836 0.18566875 0.17606455 0.08330443 0.2633508  0.2381282\n",
      "  0.18036781 0.18296862 0.4178621  0.31270486 0.718782   0.6546726\n",
      "  0.57775927 0.8699209  0.70749027 0.5330907 ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using these 100 numbers, we generate a 28x28 image (784 pixels)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "recons_image = decoder.predict(representation_sample)\r\n",
    "plt.imshow(recons_image.reshape(28,28), cmap='gray');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And here is where the *fun part* begins! using the lower-dimension representation, let's do some math."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning arithmetic operations on handwritten digits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The idea is simple. Using the representation of two images, we train a neural network to compute their sum, product and to compare them. We will not provide the value of each digit, but we will provide the results during the training step.  \r\n",
    "We will be performing addition and multiplication between numbers in the range [0-9]. The results will be in the range [0-18] and [0-81] respectively. So the results will be coded using multiple outputs:  \r\n",
    "1- Sum Units, multiclass output [0,1,2,3,4,5,6,7,8,9]  \r\n",
    "2- Sum tens, binary output [0,1]  \r\n",
    "3- Multiplication Units, multiclass output [0,1,2,3,4,5,6,7,8,9]  \r\n",
    "4- Multiplication tens, multiclass output [0,1,2,3,4,5,6,7,8]  \r\n",
    "5- Comparison, binary output [0,1]  \r\n",
    "\r\n",
    "\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/E5unC7Y.png\"> </p> \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the functional API in *KERAS* we define the network architecture. First, we import the encoder *twice* and freeze its weights:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# duplicate encoders and freeze weights\r\n",
    "encoder1 = keras.models.load_model('encoder') \r\n",
    "encoder1._name = 'encoder1'\r\n",
    "encoder1.trainable = False\r\n",
    "\r\n",
    "encoder2 = keras.models.load_model('encoder')\r\n",
    "encoder2._name = 'encoder2'\r\n",
    "encoder2.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the encoders, we build the model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# create model to learn it all\r\n",
    "input1 = Input(shape=image_dim)\r\n",
    "input2 = Input(shape=image_dim)\r\n",
    "enc1_out = encoder1(input1)\r\n",
    "enc2_out = encoder2(input2)\r\n",
    "model_c = Concatenate()([enc1_out,enc2_out])\r\n",
    "model_c = Dense(1000,activation='relu')(model_c)\r\n",
    "\r\n",
    "model_b1 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b2 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b3 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b4 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b5 = Dense(200,activation='relu')(model_c)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "units_add =  Dense(10,activation='softmax',name ='units_add')(model_b1)\r\n",
    "tens_add = Dense(1,activation='sigmoid',name ='tens_add')(model_b2)\r\n",
    "\r\n",
    "units_mult =  Dense(10,activation='softmax',name ='units_mult')(model_b3)\r\n",
    "tens_mult = Dense(9,activation='softmax',name ='tens_mult')(model_b4)\r\n",
    "\r\n",
    "comp =  Dense(1,activation='sigmoid',name ='comp')(model_b5)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "model_complete = Model(inputs=[input1,input2],outputs=[units_add,tens_add,units_mult,tens_mult,comp])\r\n",
    "\r\n",
    "model_complete.compile(optimizer='nadam', loss = ['categorical_crossentropy','binary_crossentropy','categorical_crossentropy','categorical_crossentropy','binary_crossentropy'], metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model has two inputs (the two handwritten digits images) and five outputs (units and tens of the sum and product plus the comparsion result). We will use two different losses due to the nature of the outputs. Note that there is a common hidden layer of 1000 units, and then five branches (one for each output).  \r\n",
    "We need to create datasets to train and test our model. Inputs will be random combinations of handwritten digits. Outputs will be the expected results for each combination. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# generate a dataset for additions\r\n",
    "train_size = 500000\r\n",
    "random_labels1 = np.random.randint(0,25000,train_size)\r\n",
    "random_labels2 = np.random.randint(0,25000,train_size)\r\n",
    "\r\n",
    "x_train_1 = x_train[random_labels1]\r\n",
    "x_train_2 = x_train[random_labels2]\r\n",
    "\r\n",
    "y_train_1 = y_train[random_labels1]\r\n",
    "y_train_2 = y_train[random_labels2]\r\n",
    "\r\n",
    "y_add = y_train_1 + y_train_2\r\n",
    "y_add_tens = y_add //10 \r\n",
    "y_add_units = y_add %10 \r\n",
    "y_add_units_cat = to_categorical(y_add_units)\r\n",
    "\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "test_size = 5000\r\n",
    "random_labels1 = np.random.randint(0,10000,test_size)\r\n",
    "random_labels2 = np.random.randint(0,10000,test_size)\r\n",
    "\r\n",
    "x_test_1 = x_test[random_labels1]\r\n",
    "x_test_2 = x_test[random_labels2]\r\n",
    "\r\n",
    "y_test_1 = y_test[random_labels1]\r\n",
    "y_test_2 = y_test[random_labels2]\r\n",
    "\r\n",
    "y_test_add = y_test_1 + y_test_2\r\n",
    "y_test_add_tens = y_test_add //10 \r\n",
    "y_test_add_units = y_test_add %10 \r\n",
    "y_test_add_units_cat = to_categorical(y_test_add_units)\r\n",
    "\r\n",
    "# generate a dataset for multiplication\r\n",
    "\r\n",
    "y_mult = y_train_1 * y_train_2\r\n",
    "y_mult_tens = y_mult //10 \r\n",
    "y_mult_units = y_mult %10 \r\n",
    "y_mult_units_cat = to_categorical(y_mult_units)\r\n",
    "y_mult_tens_cat = to_categorical(y_mult_tens)\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "\r\n",
    "y_test_mult = y_test_1 * y_test_2\r\n",
    "y_test_mult_tens = y_test_mult //10 \r\n",
    "y_test_mult_units = y_test_mult %10 \r\n",
    "y_test_mult_units_cat = to_categorical(y_test_mult_units)\r\n",
    "y_test_mult_tens_cat = to_categorical(y_test_mult_tens)\r\n",
    "\r\n",
    "# generate a dataset for comparison\r\n",
    "y_comp = y_train_1 > y_train_2\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "y_test_comp = y_test_1 > y_test_2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to train our model! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "history = model_complete.fit([x_train_1,x_train_2],[y_add_units_cat,y_add_tens,y_mult_units_cat,y_mult_tens_cat,y_comp], batch_size=1000,epochs=1000,validation_split=0.2, verbose=1,callbacks=[es,es,es,es,es,es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "400/400 [==============================] - 30s 75ms/step - loss: 4.4237 - units_add_loss: 1.4769 - tens_add_loss: 0.4029 - units_mult_loss: 1.1752 - tens_mult_loss: 0.9817 - comp_loss: 0.3870 - units_add_acc: 0.5030 - tens_add_acc: 0.8071 - units_mult_acc: 0.5929 - tens_mult_acc: 0.6451 - comp_acc: 0.8187 - val_loss: 2.6440 - val_units_add_loss: 0.8241 - val_tens_add_loss: 0.2876 - val_units_mult_loss: 0.6680 - val_tens_mult_loss: 0.5897 - val_comp_loss: 0.2746 - val_units_add_acc: 0.7474 - val_tens_add_acc: 0.8761 - val_units_mult_acc: 0.7851 - val_tens_mult_acc: 0.8000 - val_comp_acc: 0.8788\n",
      "Epoch 2/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 1.9248 - units_add_loss: 0.6146 - tens_add_loss: 0.1969 - units_mult_loss: 0.4890 - tens_mult_loss: 0.4288 - comp_loss: 0.1955 - units_add_acc: 0.8112 - tens_add_acc: 0.9229 - units_mult_acc: 0.8462 - tens_mult_acc: 0.8596 - comp_acc: 0.9230 - val_loss: 1.4771 - val_units_add_loss: 0.4828 - val_tens_add_loss: 0.1505 - val_units_mult_loss: 0.3745 - val_tens_mult_loss: 0.3205 - val_comp_loss: 0.1489 - val_units_add_acc: 0.8526 - val_tens_add_acc: 0.9436 - val_units_mult_acc: 0.8830 - val_tens_mult_acc: 0.8971 - val_comp_acc: 0.9440\n",
      "Epoch 3/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 1.2749 - units_add_loss: 0.4142 - tens_add_loss: 0.1309 - units_mult_loss: 0.3203 - tens_mult_loss: 0.2797 - comp_loss: 0.1298 - units_add_acc: 0.8747 - tens_add_acc: 0.9521 - units_mult_acc: 0.9012 - tens_mult_acc: 0.9105 - comp_acc: 0.9517 - val_loss: 1.1615 - val_units_add_loss: 0.3635 - val_tens_add_loss: 0.1507 - val_units_mult_loss: 0.2710 - val_tens_mult_loss: 0.2620 - val_comp_loss: 0.1145 - val_units_add_acc: 0.8867 - val_tens_add_acc: 0.9415 - val_units_mult_acc: 0.9139 - val_tens_mult_acc: 0.9128 - val_comp_acc: 0.9584\n",
      "Epoch 4/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 0.9157 - units_add_loss: 0.2978 - tens_add_loss: 0.0943 - units_mult_loss: 0.2280 - tens_mult_loss: 0.2014 - comp_loss: 0.0942 - units_add_acc: 0.9102 - tens_add_acc: 0.9665 - units_mult_acc: 0.9298 - tens_mult_acc: 0.9359 - comp_acc: 0.9663 - val_loss: 0.7949 - val_units_add_loss: 0.2587 - val_tens_add_loss: 0.0841 - val_units_mult_loss: 0.1957 - val_tens_mult_loss: 0.1753 - val_comp_loss: 0.0811 - val_units_add_acc: 0.9218 - val_tens_add_acc: 0.9705 - val_units_mult_acc: 0.9388 - val_tens_mult_acc: 0.9441 - val_comp_acc: 0.9714\n",
      "Epoch 5/1000\n",
      "400/400 [==============================] - 29s 74ms/step - loss: 0.6795 - units_add_loss: 0.2184 - tens_add_loss: 0.0709 - units_mult_loss: 0.1685 - tens_mult_loss: 0.1496 - comp_loss: 0.0721 - units_add_acc: 0.9341 - tens_add_acc: 0.9751 - units_mult_acc: 0.9483 - tens_mult_acc: 0.9523 - comp_acc: 0.9747 - val_loss: 0.6265 - val_units_add_loss: 0.2025 - val_tens_add_loss: 0.0655 - val_units_mult_loss: 0.1523 - val_tens_mult_loss: 0.1399 - val_comp_loss: 0.0663 - val_units_add_acc: 0.9376 - val_tens_add_acc: 0.9773 - val_units_mult_acc: 0.9522 - val_tens_mult_acc: 0.9542 - val_comp_acc: 0.9761\n",
      "Epoch 6/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 0.5089 - units_add_loss: 0.1611 - tens_add_loss: 0.0539 - units_mult_loss: 0.1258 - tens_mult_loss: 0.1127 - comp_loss: 0.0553 - units_add_acc: 0.9518 - tens_add_acc: 0.9816 - units_mult_acc: 0.9615 - tens_mult_acc: 0.9646 - comp_acc: 0.9809 - val_loss: 0.4843 - val_units_add_loss: 0.1541 - val_tens_add_loss: 0.0515 - val_units_mult_loss: 0.1219 - val_tens_mult_loss: 0.1051 - val_comp_loss: 0.0517 - val_units_add_acc: 0.9534 - val_tens_add_acc: 0.9819 - val_units_mult_acc: 0.9628 - val_tens_mult_acc: 0.9668 - val_comp_acc: 0.9819\n",
      "Epoch 7/1000\n",
      "400/400 [==============================] - 30s 74ms/step - loss: 0.3879 - units_add_loss: 0.1213 - tens_add_loss: 0.0420 - units_mult_loss: 0.0952 - tens_mult_loss: 0.0859 - comp_loss: 0.0435 - units_add_acc: 0.9642 - tens_add_acc: 0.9860 - units_mult_acc: 0.9710 - tens_mult_acc: 0.9732 - comp_acc: 0.9855 - val_loss: 0.4573 - val_units_add_loss: 0.1453 - val_tens_add_loss: 0.0552 - val_units_mult_loss: 0.1067 - val_tens_mult_loss: 0.0979 - val_comp_loss: 0.0521 - val_units_add_acc: 0.9540 - val_tens_add_acc: 0.9795 - val_units_mult_acc: 0.9665 - val_tens_mult_acc: 0.9674 - val_comp_acc: 0.9809\n",
      "Epoch 8/1000\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 0.2896 - units_add_loss: 0.0894 - tens_add_loss: 0.0320 - units_mult_loss: 0.0706 - tens_mult_loss: 0.0642 - comp_loss: 0.0334 - units_add_acc: 0.9740 - tens_add_acc: 0.9896 - units_mult_acc: 0.9791 - tens_mult_acc: 0.9801 - comp_acc: 0.9888 - val_loss: 0.3066 - val_units_add_loss: 0.0952 - val_tens_add_loss: 0.0341 - val_units_mult_loss: 0.0746 - val_tens_mult_loss: 0.0673 - val_comp_loss: 0.0355 - val_units_add_acc: 0.9712 - val_tens_add_acc: 0.9884 - val_units_mult_acc: 0.9773 - val_tens_mult_acc: 0.9786 - val_comp_acc: 0.9878\n",
      "Epoch 9/1000\n",
      "400/400 [==============================] - 31s 77ms/step - loss: 0.2181 - units_add_loss: 0.0658 - tens_add_loss: 0.0250 - units_mult_loss: 0.0529 - tens_mult_loss: 0.0483 - comp_loss: 0.0261 - units_add_acc: 0.9815 - tens_add_acc: 0.9920 - units_mult_acc: 0.9845 - tens_mult_acc: 0.9855 - comp_acc: 0.9917 - val_loss: 0.2575 - val_units_add_loss: 0.0790 - val_tens_add_loss: 0.0296 - val_units_mult_loss: 0.0636 - val_tens_mult_loss: 0.0565 - val_comp_loss: 0.0289 - val_units_add_acc: 0.9752 - val_tens_add_acc: 0.9898 - val_units_mult_acc: 0.9802 - val_tens_mult_acc: 0.9819 - val_comp_acc: 0.9899\n",
      "Epoch 10/1000\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.7182 - units_add_loss: 0.2014 - tens_add_loss: 0.1066 - units_mult_loss: 0.1641 - tens_mult_loss: 0.1737 - comp_loss: 0.0724 - units_add_acc: 0.9521 - tens_add_acc: 0.9788 - units_mult_acc: 0.9612 - tens_mult_acc: 0.9635 - comp_acc: 0.9795Restoring model weights from the end of the best epoch.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "400/400 [==============================] - 29s 72ms/step - loss: 0.7182 - units_add_loss: 0.2014 - tens_add_loss: 0.1066 - units_mult_loss: 0.1641 - tens_mult_loss: 0.1737 - comp_loss: 0.0724 - units_add_acc: 0.9521 - tens_add_acc: 0.9788 - units_mult_acc: 0.9612 - tens_mult_acc: 0.9635 - comp_acc: 0.9795 - val_loss: 0.5373 - val_units_add_loss: 0.1700 - val_tens_add_loss: 0.0588 - val_units_mult_loss: 0.1300 - val_tens_mult_loss: 0.1199 - val_comp_loss: 0.0587 - val_units_add_acc: 0.9486 - val_tens_add_acc: 0.9797 - val_units_mult_acc: 0.9592 - val_tens_mult_acc: 0.9621 - val_comp_acc: 0.9797\n",
      "Epoch 00010: early stopping\n",
      "Epoch 00010: early stopping\n",
      "Epoch 00010: early stopping\n",
      "Epoch 00010: early stopping\n",
      "Epoch 00010: early stopping\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of the training, the accuracy on all outputs is pretty good (9x%). Let's see first how the model performs on the test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "test = model_complete.evaluate([x_test_1,x_test_2],[y_test_add_units_cat,y_test_add_tens,y_test_mult_units_cat,y_test_mult_tens_cat,y_test_comp], batch_size=100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50/50 [==============================] - 1s 12ms/step - loss: 0.6568 - units_add_loss: 0.2018 - tens_add_loss: 0.0821 - units_mult_loss: 0.1564 - tens_mult_loss: 0.1448 - comp_loss: 0.0717 - units_add_acc: 0.9378 - tens_add_acc: 0.9742 - units_mult_acc: 0.9522 - tens_mult_acc: 0.9536 - comp_acc: 0.9762\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results are still in the 9x%. We can show a random sample of the model predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "random_label_1 = np.random.randint(0,9999)\r\n",
    "random_label_2 = np.random.randint(0,9999)\r\n",
    "\r\n",
    "img_sample1 = x_test[random_label_1,:,:].reshape((1,28,28,1))\r\n",
    "img_sample2 = x_test[random_label_2,:,:].reshape((1,28,28,1))\r\n",
    "\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.imshow(img_sample1.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.imshow(img_sample2.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "prediction = model_complete.predict([img_sample1,img_sample2])\r\n",
    "unit_add = prediction[0]\r\n",
    "ten_add = prediction[1]\r\n",
    "unit_mult = prediction[2]\r\n",
    "ten_mult = prediction[3]\r\n",
    "comp_images = prediction[4]\r\n",
    "\r\n",
    "sum_images = np.argmax(unit_add)+10*np.round(ten_add)\r\n",
    "print('sum =',sum_images)\r\n",
    "\r\n",
    "\r\n",
    "mult_images = np.argmax(unit_mult)+10*np.argmax(ten_mult)\r\n",
    "print('product =',mult_images)\r\n",
    "\r\n",
    "print('comparison result =',np.round(comp_images),'(1 if the number on the left is greater, 0 elsewhere)')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sum = [[12.]]\n",
      "product = 35\n",
      "comparison result = [[0.]] (1 if the number on the left is greater, 0 elsewhere)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results look promising! We actually could improve the accuracy by training the model on more random samples (increase `train_size` value) or tweak the model architecture. One last thing: save the model!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# save the model\r\n",
    "model_complete.save('model_complete')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion and future work"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "684b1123683431d89d3bfe9a89cc763215f4b8cd94b4aba1fb40ad45ff7c8b41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}