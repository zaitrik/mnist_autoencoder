{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This is how a neural network learns to add, multiply and compare handwritten digits WITHOUT knowing their values "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\"> <img src=\"https://i.dlpng.com/static/png/6906777_preview.png\"> </p>   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I described in a [previous post](https://blog.jovian.ai/how-to-train-supervised-machine-learning-algorithms-without-labeled-data-6ebddc01a00f), how useful are autoencoders in  automated labeling. The main property of these networks is their ability to learn features/patterns in the data. This is in fact not specific to autoencoders and can be implemented using other unsupervised techniques, mainly **PCA**.  \r\n",
    "The ability to detect and learn features in data can be used in other areas.  \r\n",
    "\r\n",
    "In this post, I will present some applications of convolutional autoencoders:  \r\n",
    "- First, a convolutional autoencoder will be trained on **MNIST** data.\r\n",
    "- After the training of the encoder and decoder, we will freeze their weights and use them with additional dense layers to \"learn\" arithmetic operations, namely addition, multiplication and comparison.  \r\n",
    "The trick is to *never* explicitly associate the handwritten digits in **MNIST** dataset with their respective labels. We will see that the neural networks will be nevertheless able to reach 97+% accuracy in all cases on unseen data.\r\n",
    "\r\n",
    "The first step of the design is described in the following diagram:\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/chLUEdp.png\"> </p>   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the second step, we will use the encoder in series with dense layers to perform arithmetic operations: addition, multiplication and comparison. We will train only the dense layer weights, and supply the results of the operations as labels. note that we will not supply the digits values (labels).\r\n",
    "\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/s8U8up4.png\"> </p> \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training an autoencoder on MNIST data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar to the previous article, we will use MNIST data in this experiment. The autoencoder will learn the handwritten digits features using 60000 training samples. We import MNIST using *KERAS* library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#import libraries and setup \r\n",
    "import keras\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import logging\r\n",
    "logging.getLogger('tensorflow').disabled = True\r\n",
    "from keras.models import Sequential, Model\r\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, UpSampling2D, Reshape, Concatenate, Input\r\n",
    "from keras.callbacks import EarlyStopping\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, restore_best_weights=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import mnist\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n",
    "print(x_train.shape,y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We scale the data in the range `[0,1]` and reshape it to *KERAS* format for images (nbr_samples x width x height x channels) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#normalize data\r\n",
    "if x_train.max() >1:\r\n",
    "    x_train = x_train / 255\r\n",
    "    x_test = x_test / 255\r\n",
    "\r\n",
    "default_shape = x_train.shape\r\n",
    "#reshape input data to 1 channel\r\n",
    "x_train = x_train.reshape(-1,default_shape[1],default_shape[2],1)\r\n",
    "x_test = x_test.reshape(-1,default_shape[1],default_shape[2],1)\r\n",
    "image_dim = x_train.shape[1:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will implement a similar autoencoder architecture as in [my previous post](https://blog.jovian.ai/how-to-train-supervised-machine-learning-algorithms-without-labeled-data-6ebddc01a00f). It is based on a series of convolutional layers, that will gradually encode the 28x28x1 image (784 pixel) into a 100 elements array, and decode that representation back to the original format. The resulting image -after the training- will hopefully resemble to the original one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# create an autoencoder / decoder \r\n",
    "encoder = Sequential()\r\n",
    "encoder.add(Conv2D(32,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu',input_shape=image_dim))\r\n",
    "encoder.add(MaxPooling2D(2,2))\r\n",
    "encoder.add(Conv2D(64,kernel_size=(3,3), strides=(1,1),padding='same',activation='selu'))\r\n",
    "encoder.add(MaxPooling2D(2,2))\r\n",
    "encoder.add(Conv2D(128,kernel_size=(3,3), strides=(1,1),padding='same',activation='selu'))\r\n",
    "encoder.add(Flatten())\r\n",
    "encoder.add(Dense(100,activation='sigmoid'))\r\n",
    "encoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               627300    \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 719,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "encoder_out_dim = encoder.layers[-1].output_shape[1:] # dimension of the encoder output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "decoder = Sequential()\r\n",
    "decoder.add(Dense(6272, activation='sigmoid', input_shape=encoder_out_dim))\r\n",
    "decoder.add(Reshape(( 7, 7, 128)))\r\n",
    "decoder.add(Conv2D(128,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu'))\r\n",
    "decoder.add(UpSampling2D((2,2)))\r\n",
    "decoder.add(Conv2D(64,kernel_size=(3,3), strides=(1,1),padding='same', activation='selu'))\r\n",
    "decoder.add(UpSampling2D((2,2)))\r\n",
    "decoder.add(Conv2D(1,kernel_size=(3,3), strides=(1,1),padding='same', activation='sigmoid'))\r\n",
    "\r\n",
    "decoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "=================================================================\n",
      "Total params: 855,425\n",
      "Trainable params: 855,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The autoencoder is then created using the encoder and the decoder:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "enc_dec = Sequential([encoder,decoder])\r\n",
    "enc_dec.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 100)               719972    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 28, 28, 1)         855425    \n",
      "=================================================================\n",
      "Total params: 1,575,397\n",
      "Trainable params: 1,575,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each autoencoder output will be trained as a binary classifier for each pixel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "enc_dec.compile(optimizer='nadam', loss = 'binary_crossentropy')\r\n",
    "history = enc_dec.fit(x_train,x_train, batch_size=100,epochs=1000,validation_split=0.2, verbose=2,callbacks=[es,es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "480/480 - 14s - loss: 0.2806 - val_loss: 0.1985\n",
      "Epoch 2/1000\n",
      "480/480 - 14s - loss: 0.1468 - val_loss: 0.1151\n",
      "Epoch 3/1000\n",
      "480/480 - 14s - loss: 0.1027 - val_loss: 0.0907\n",
      "Epoch 4/1000\n",
      "480/480 - 14s - loss: 0.0875 - val_loss: 0.0849\n",
      "Epoch 5/1000\n",
      "480/480 - 14s - loss: 0.0815 - val_loss: 0.0790\n",
      "Epoch 6/1000\n",
      "480/480 - 14s - loss: 0.0781 - val_loss: 0.0767\n",
      "Epoch 7/1000\n",
      "480/480 - 14s - loss: 0.0758 - val_loss: 0.0759\n",
      "Epoch 8/1000\n",
      "480/480 - 14s - loss: 0.0744 - val_loss: 0.0758\n",
      "Epoch 9/1000\n",
      "480/480 - 14s - loss: 0.0731 - val_loss: 0.0725\n",
      "Epoch 10/1000\n",
      "480/480 - 14s - loss: 0.0722 - val_loss: 0.0716\n",
      "Epoch 11/1000\n",
      "480/480 - 14s - loss: 0.0713 - val_loss: 0.0720\n",
      "Epoch 12/1000\n",
      "480/480 - 14s - loss: 0.0707 - val_loss: 0.0718\n",
      "Epoch 13/1000\n",
      "480/480 - 15s - loss: 0.0702 - val_loss: 0.0731\n",
      "Epoch 14/1000\n",
      "480/480 - 15s - loss: 0.0696 - val_loss: 0.0702\n",
      "Epoch 15/1000\n",
      "480/480 - 14s - loss: 0.0692 - val_loss: 0.0690\n",
      "Epoch 16/1000\n",
      "480/480 - 14s - loss: 0.0687 - val_loss: 0.0695\n",
      "Epoch 17/1000\n",
      "480/480 - 15s - loss: 0.0684 - val_loss: 0.0684\n",
      "Epoch 18/1000\n",
      "480/480 - 14s - loss: 0.0680 - val_loss: 0.0699\n",
      "Epoch 19/1000\n",
      "480/480 - 14s - loss: 0.0678 - val_loss: 0.0691\n",
      "Epoch 20/1000\n",
      "480/480 - 14s - loss: 0.0675 - val_loss: 0.0681\n",
      "Epoch 21/1000\n",
      "480/480 - 14s - loss: 0.0673 - val_loss: 0.0674\n",
      "Epoch 22/1000\n",
      "480/480 - 14s - loss: 0.0670 - val_loss: 0.0674\n",
      "Epoch 23/1000\n",
      "480/480 - 14s - loss: 0.0668 - val_loss: 0.0686\n",
      "Epoch 24/1000\n",
      "480/480 - 14s - loss: 0.1794 - val_loss: 0.1077\n",
      "Epoch 25/1000\n",
      "480/480 - 14s - loss: 0.0960 - val_loss: 0.0813\n",
      "Epoch 26/1000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "480/480 - 14s - loss: 0.0752 - val_loss: 0.0718\n",
      "Epoch 00026: early stopping\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The early stopping will make sure the autoencoder will not overfit the training data. There are two ways to check the network performance. First, we can evaluate the loss function on test data, and expect it to be close to the loss value on the training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "enc_dec.evaluate(x_test,x_test,batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10/10 [==============================] - 1s 66ms/step - loss: 0.0706\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.07061594724655151"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "enc_dec.evaluate(x_train,x_train,batch_size=1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60/60 [==============================] - 4s 72ms/step - loss: 0.0710\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0709829330444336"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Values are very close for both data sets. The second method is to check the resulting reconstitution that we obtain for a random sample from the test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "random_label = np.random.randint(0,9999)\r\n",
    "img_sample = x_test[random_label,:,:].reshape((1,28,28,1))\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.imshow(img_sample.reshape(28,28), cmap='gray');\r\n",
    "plt.title('Original image');\r\n",
    "pred_img = enc_dec.predict(img_sample) \r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.imshow(pred_img.reshape(28,28), cmap='gray');\r\n",
    "plt.title('Reconstructed image');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*A picture is worth a thousand words!* The reconstitution is very close to the original image."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# save models\r\n",
    "encoder.save('encoder')\r\n",
    "decoder.save('decoder')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# load models if already trained\r\n",
    "#encoder = keras.models.load_model('encoder') \r\n",
    "#decoder = keras.models.load_model('decoder') \r\n",
    "#enc_dec = Sequential([encoder,decoder])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a trained encoder and decoder, let's focus on the *encoder*. For each image, the encoder generates a representation that captures the most \"interesting\" or \"important\" features. This representation is sufficient to reconstitute the image using the decoder. Here is the representation of the sample image we used earlier: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "representation_sample = encoder.predict(img_sample)\r\n",
    "print(representation_sample) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.6130281  0.76459074 0.6310103  0.04438157 0.10678293 0.13682882\n",
      "  0.28006715 0.39419928 0.7018184  0.41447732 0.32862127 0.17540242\n",
      "  0.2009563  0.3483818  0.87368184 0.35199907 0.65021783 0.46785977\n",
      "  0.14121868 0.23320875 0.5732315  0.78764004 0.6864616  0.16232853\n",
      "  0.77741575 0.3470657  0.6527449  0.5624531  0.6275401  0.86655647\n",
      "  0.4702597  0.9197414  0.84741426 0.76422065 0.66112906 0.8598242\n",
      "  0.4149059  0.12067313 0.72827107 0.9057333  0.09169123 0.785913\n",
      "  0.15651442 0.8360928  0.1388307  0.18954395 0.61363137 0.4155853\n",
      "  0.3886866  0.3826588  0.10800058 0.92811954 0.16767563 0.13634066\n",
      "  0.6332908  0.80491865 0.3511647  0.19888486 0.07157111 0.8211692\n",
      "  0.24735563 0.33792862 0.75648785 0.4793063  0.3792692  0.32980424\n",
      "  0.89582556 0.4915181  0.29687124 0.62603694 0.4230361  0.35062855\n",
      "  0.51796675 0.631057   0.30978802 0.7018079  0.84298    0.37712884\n",
      "  0.6510555  0.90781754 0.53740066 0.26541686 0.10492547 0.28378004\n",
      "  0.514947   0.8901576  0.12974055 0.5410253  0.1769872  0.47479823\n",
      "  0.08268451 0.16334929 0.54698175 0.74747974 0.27949828 0.13411115\n",
      "  0.22839272 0.35881448 0.15508696 0.66591793]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using these 100 numbers, we generate a 28x28 image (784 pixels)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "recons_image = decoder.predict(representation_sample)\r\n",
    "plt.imshow(recons_image.reshape(28,28), cmap='gray');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And here is where the *fun part* begins! using the lower-dimension representation, let's do some math."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning arithmetic operations on handwritten digits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The idea is simple. Using the representation of two images, we train a neural network to compute their sum, their product and to compare them. We will not provide the value of each digit, but we will provide the results during the training step.  \r\n",
    "We will be performing addition and multiplication between numbers in the range [0-9]. The results will be in the range [0-18] and [0-81] respectively. So the results will be coded using multiple outputs:  \r\n",
    "1- Sum **units**, multiclass output [0,1,2,3,4,5,6,7,8,9]  \r\n",
    "2- Sum **tens**, binary output [0,1]  \r\n",
    "3- Multiplication **units**, multiclass output [0,1,2,3,4,5,6,7,8,9]  \r\n",
    "4- Multiplication **tens**, multiclass output [0,1,2,3,4,5,6,7,8]  \r\n",
    "5- Comparison result, binary output [0,1]  \r\n",
    "\r\n",
    "\r\n",
    "<p align=\"center\"> <img src=\"https://i.imgur.com/E5unC7Y.png\"> </p> \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the functional API in *KERAS* we define the network architecture. First, we import the encoder *twice* and freeze its weights:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# duplicate encoders and freeze weights\r\n",
    "encoder1 = keras.models.load_model('encoder') \r\n",
    "encoder1._name = 'encoder1'\r\n",
    "encoder1.trainable = False\r\n",
    "\r\n",
    "encoder2 = keras.models.load_model('encoder')\r\n",
    "encoder2._name = 'encoder2'\r\n",
    "encoder2.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the encoders outputs, we build the model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# create model to learn it all\r\n",
    "input1 = Input(shape=image_dim)\r\n",
    "input2 = Input(shape=image_dim)\r\n",
    "enc1_out = encoder1(input1)\r\n",
    "enc2_out = encoder2(input2)\r\n",
    "model_c = Concatenate()([enc1_out,enc2_out])\r\n",
    "model_c = Dense(1000,activation='relu')(model_c)\r\n",
    "\r\n",
    "model_b1 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b2 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b3 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b4 = Dense(200,activation='relu')(model_c)\r\n",
    "model_b5 = Dense(200,activation='relu')(model_c)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "units_add =  Dense(10,activation='softmax',name ='units_add')(model_b1)\r\n",
    "tens_add = Dense(1,activation='sigmoid',name ='tens_add')(model_b2)\r\n",
    "\r\n",
    "units_mult =  Dense(10,activation='softmax',name ='units_mult')(model_b3)\r\n",
    "tens_mult = Dense(9,activation='softmax',name ='tens_mult')(model_b4)\r\n",
    "\r\n",
    "comp =  Dense(1,activation='sigmoid',name ='comp')(model_b5)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "model_complete = Model(inputs=[input1,input2],outputs=[units_add,tens_add,units_mult,tens_mult,comp])\r\n",
    "\r\n",
    "model_complete.compile(optimizer='nadam', loss = ['categorical_crossentropy','binary_crossentropy','categorical_crossentropy','categorical_crossentropy','binary_crossentropy'], metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model has two inputs (the two handwritten digits images) and five outputs (units and tens of the sum and product plus the comparsion result). We will use two different losses due to the nature of the outputs. Note that there is a common hidden layer of 1000 units, and then five branches (one for each output).  \r\n",
    "We need to create datasets to train and test our model. Inputs will be random combinations of handwritten digits. Outputs will be the expected results for each combination. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# generate a dataset for additions\r\n",
    "train_size = 1000000\r\n",
    "random_labels1 = np.random.randint(0,25000,train_size)\r\n",
    "random_labels2 = np.random.randint(0,25000,train_size)\r\n",
    "\r\n",
    "x_train_1 = x_train[random_labels1]\r\n",
    "x_train_2 = x_train[random_labels2]\r\n",
    "\r\n",
    "y_train_1 = y_train[random_labels1]\r\n",
    "y_train_2 = y_train[random_labels2]\r\n",
    "\r\n",
    "y_add = y_train_1 + y_train_2\r\n",
    "y_add_tens = y_add //10 \r\n",
    "y_add_units = y_add %10 \r\n",
    "y_add_units_cat = to_categorical(y_add_units)\r\n",
    "\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "test_size = 5000\r\n",
    "random_labels1 = np.random.randint(0,10000,test_size)\r\n",
    "random_labels2 = np.random.randint(0,10000,test_size)\r\n",
    "\r\n",
    "x_test_1 = x_test[random_labels1]\r\n",
    "x_test_2 = x_test[random_labels2]\r\n",
    "\r\n",
    "y_test_1 = y_test[random_labels1]\r\n",
    "y_test_2 = y_test[random_labels2]\r\n",
    "\r\n",
    "y_test_add = y_test_1 + y_test_2\r\n",
    "y_test_add_tens = y_test_add //10 \r\n",
    "y_test_add_units = y_test_add %10 \r\n",
    "y_test_add_units_cat = to_categorical(y_test_add_units)\r\n",
    "\r\n",
    "# generate a dataset for multiplication\r\n",
    "\r\n",
    "y_mult = y_train_1 * y_train_2\r\n",
    "y_mult_tens = y_mult //10 \r\n",
    "y_mult_units = y_mult %10 \r\n",
    "y_mult_units_cat = to_categorical(y_mult_units)\r\n",
    "y_mult_tens_cat = to_categorical(y_mult_tens)\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "\r\n",
    "y_test_mult = y_test_1 * y_test_2\r\n",
    "y_test_mult_tens = y_test_mult //10 \r\n",
    "y_test_mult_units = y_test_mult %10 \r\n",
    "y_test_mult_units_cat = to_categorical(y_test_mult_units)\r\n",
    "y_test_mult_tens_cat = to_categorical(y_test_mult_tens)\r\n",
    "\r\n",
    "# generate a dataset for comparison\r\n",
    "y_comp = y_train_1 > y_train_2\r\n",
    "\r\n",
    "# the same with x_test\r\n",
    "y_test_comp = y_test_1 > y_test_2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to train our model! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "history = model_complete.fit([x_train_1,x_train_2],[y_add_units_cat,y_add_tens,y_mult_units_cat,y_mult_tens_cat,y_comp], batch_size=1000,epochs=1000,validation_split=0.2, verbose=1,callbacks=[es,es,es,es,es,es])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "800/800 [==============================] - 87s 108ms/step - loss: 3.0021 - units_add_loss: 0.9774 - tens_add_loss: 0.2878 - units_mult_loss: 0.7837 - tens_mult_loss: 0.6782 - comp_loss: 0.2750 - units_add_acc: 0.6821 - tens_add_acc: 0.8709 - units_mult_acc: 0.7364 - tens_mult_acc: 0.7621 - comp_acc: 0.8787 - val_loss: 1.4618 - val_units_add_loss: 0.4590 - val_tens_add_loss: 0.1448 - val_units_mult_loss: 0.3579 - val_tens_mult_loss: 0.3241 - val_comp_loss: 0.1760 - val_units_add_acc: 0.8583 - val_tens_add_acc: 0.9457 - val_units_mult_acc: 0.8872 - val_tens_mult_acc: 0.8933 - val_comp_acc: 0.9290\n",
      "Epoch 2/1000\n",
      "800/800 [==============================] - 62s 77ms/step - loss: 0.9609 - units_add_loss: 0.3048 - tens_add_loss: 0.1016 - units_mult_loss: 0.2396 - tens_mult_loss: 0.2169 - comp_loss: 0.0980 - units_add_acc: 0.9079 - tens_add_acc: 0.9636 - units_mult_acc: 0.9261 - tens_mult_acc: 0.9308 - comp_acc: 0.9647 - val_loss: 0.6533 - val_units_add_loss: 0.2059 - val_tens_add_loss: 0.0712 - val_units_mult_loss: 0.1609 - val_tens_mult_loss: 0.1499 - val_comp_loss: 0.0655 - val_units_add_acc: 0.9388 - val_tens_add_acc: 0.9760 - val_units_mult_acc: 0.9516 - val_tens_mult_acc: 0.9534 - val_comp_acc: 0.9774\n",
      "Epoch 3/1000\n",
      "800/800 [==============================] - 62s 77ms/step - loss: 0.4979 - units_add_loss: 0.1546 - tens_add_loss: 0.0550 - units_mult_loss: 0.1218 - tens_mult_loss: 0.1133 - comp_loss: 0.0532 - units_add_acc: 0.9539 - tens_add_acc: 0.9811 - units_mult_acc: 0.9629 - tens_mult_acc: 0.9645 - comp_acc: 0.9817 - val_loss: 0.3717 - val_units_add_loss: 0.1145 - val_tens_add_loss: 0.0418 - val_units_mult_loss: 0.0900 - val_tens_mult_loss: 0.0848 - val_comp_loss: 0.0407 - val_units_add_acc: 0.9665 - val_tens_add_acc: 0.9858 - val_units_mult_acc: 0.9727 - val_tens_mult_acc: 0.9738 - val_comp_acc: 0.9863\n",
      "Epoch 4/1000\n",
      "800/800 [==============================] - 62s 77ms/step - loss: 0.2767 - units_add_loss: 0.0827 - tens_add_loss: 0.0325 - units_mult_loss: 0.0665 - tens_mult_loss: 0.0635 - comp_loss: 0.0315 - units_add_acc: 0.9759 - tens_add_acc: 0.9892 - units_mult_acc: 0.9801 - tens_mult_acc: 0.9805 - comp_acc: 0.9897 - val_loss: 0.2249 - val_units_add_loss: 0.0658 - val_tens_add_loss: 0.0277 - val_units_mult_loss: 0.0535 - val_tens_mult_loss: 0.0522 - val_comp_loss: 0.0256 - val_units_add_acc: 0.9811 - val_tens_add_acc: 0.9909 - val_units_mult_acc: 0.9838 - val_tens_mult_acc: 0.9841 - val_comp_acc: 0.9913\n",
      "Epoch 5/1000\n",
      "800/800 [==============================] - 62s 78ms/step - loss: 0.1507 - units_add_loss: 0.0426 - tens_add_loss: 0.0192 - units_mult_loss: 0.0351 - tens_mult_loss: 0.0352 - comp_loss: 0.0186 - units_add_acc: 0.9884 - tens_add_acc: 0.9940 - units_mult_acc: 0.9902 - tens_mult_acc: 0.9899 - comp_acc: 0.9942 - val_loss: 0.1324 - val_units_add_loss: 0.0367 - val_tens_add_loss: 0.0173 - val_units_mult_loss: 0.0308 - val_tens_mult_loss: 0.0314 - val_comp_loss: 0.0163 - val_units_add_acc: 0.9895 - val_tens_add_acc: 0.9944 - val_units_mult_acc: 0.9912 - val_tens_mult_acc: 0.9906 - val_comp_acc: 0.9948\n",
      "Epoch 6/1000\n",
      "800/800 [==============================] - 62s 77ms/step - loss: 0.0796 - units_add_loss: 0.0208 - tens_add_loss: 0.0111 - units_mult_loss: 0.0180 - tens_mult_loss: 0.0189 - comp_loss: 0.0107 - units_add_acc: 0.9952 - tens_add_acc: 0.9968 - units_mult_acc: 0.9956 - tens_mult_acc: 0.9952 - comp_acc: 0.9970 - val_loss: 0.0738 - val_units_add_loss: 0.0188 - val_tens_add_loss: 0.0108 - val_units_mult_loss: 0.0165 - val_tens_mult_loss: 0.0177 - val_comp_loss: 0.0100 - val_units_add_acc: 0.9952 - val_tens_add_acc: 0.9966 - val_units_mult_acc: 0.9956 - val_tens_mult_acc: 0.9950 - val_comp_acc: 0.9971\n",
      "Epoch 7/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.0878 - units_add_loss: 0.0203 - tens_add_loss: 0.0147 - units_mult_loss: 0.0169 - tens_mult_loss: 0.0266 - comp_loss: 0.0093 - units_add_acc: 0.9972 - tens_add_acc: 0.9976 - units_mult_acc: 0.9974 - tens_mult_acc: 0.9970 - comp_acc: 0.9980Restoring model weights from the end of the best epoch.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "800/800 [==============================] - 62s 78ms/step - loss: 0.0878 - units_add_loss: 0.0203 - tens_add_loss: 0.0147 - units_mult_loss: 0.0169 - tens_mult_loss: 0.0266 - comp_loss: 0.0093 - units_add_acc: 0.9972 - tens_add_acc: 0.9976 - units_mult_acc: 0.9974 - tens_mult_acc: 0.9970 - comp_acc: 0.9980 - val_loss: 146.7526 - val_units_add_loss: 18.1009 - val_tens_add_loss: 55.4046 - val_units_mult_loss: 19.4146 - val_tens_mult_loss: 51.7461 - val_comp_loss: 2.0863 - val_units_add_acc: 0.1281 - val_tens_add_acc: 0.4388 - val_units_mult_acc: 0.1431 - val_tens_mult_acc: 0.1329 - val_comp_acc: 0.6834\n",
      "Epoch 00007: early stopping\n",
      "Epoch 00007: early stopping\n",
      "Epoch 00007: early stopping\n",
      "Epoch 00007: early stopping\n",
      "Epoch 00007: early stopping\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of the training, the accuracy on all outputs is pretty good (9x%). Let's see first how the model performs on the test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "test = model_complete.evaluate([x_test_1,x_test_2],[y_test_add_units_cat,y_test_add_tens,y_test_mult_units_cat,y_test_mult_tens_cat,y_test_comp], batch_size=100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50/50 [==============================] - 1s 13ms/step - loss: 0.5376 - units_add_loss: 0.1653 - tens_add_loss: 0.0632 - units_mult_loss: 0.1330 - tens_mult_loss: 0.1072 - comp_loss: 0.0688 - units_add_acc: 0.9536 - tens_add_acc: 0.9800 - units_mult_acc: 0.9626 - tens_mult_acc: 0.9678 - comp_acc: 0.9802\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results are still in the 95+% range. Let's show a random sample of the model predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "random_index_1 = np.random.randint(0,9999)\r\n",
    "random_index_2 = np.random.randint(0,9999)\r\n",
    "\r\n",
    "img_sample1 = x_test[random_index_1,:,:].reshape((1,28,28,1))\r\n",
    "img_sample2 = x_test[random_index_2,:,:].reshape((1,28,28,1))\r\n",
    "\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.imshow(img_sample1.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.imshow(img_sample2.reshape(28,28), cmap='gray');\r\n",
    "\r\n",
    "prediction = model_complete.predict([img_sample1,img_sample2])\r\n",
    "unit_add = prediction[0]\r\n",
    "ten_add = prediction[1]\r\n",
    "unit_mult = prediction[2]\r\n",
    "ten_mult = prediction[3]\r\n",
    "comp_images = prediction[4]\r\n",
    "\r\n",
    "sum_images = np.argmax(unit_add)+10*np.round(ten_add)\r\n",
    "print('sum =',sum_images)\r\n",
    "\r\n",
    "\r\n",
    "mult_images = np.argmax(unit_mult)+10*np.argmax(ten_mult)\r\n",
    "print('product =',mult_images)\r\n",
    "\r\n",
    "print('comparison result =',np.round(comp_images),'(1 if the number on the left is greater, 0 elsewhere)')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sum = [[10.]]\n",
      "product = 16\n",
      "comparison result = [[0.]] (1 if the number on the left is greater, 0 elsewhere)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could improve the accuracy by training the model on more random samples (increase `train_size` value) or tweak the model architecture. With that being said, we managed to build a neural network that is capable of solving basic arithmetic operations on handwritten digits *without* explicitly computing their values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# save the model\r\n",
    "model_complete.save('model_complete')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion and future work"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this post, we designed an autoencoder that we trained on MNIST images. A neural network, based on the autoencoder was trained to perform arithmetic operations. During the autoencoder training, the **encoder** learns the most important features of the images, in order to reconstruct them later via the **decoder**. These features can be used in further operations (usually using dense or recurrent layers). We achieved more than 95% accuracy on all outputs.  \r\n",
    "\r\n",
    "Autoencoders are part of unsupervised learning. We are still scratching the surface of these amazing machine learning techniques. I will continue to explore this area, especially using recurrent neural networks and their applications in natural language processing and time series.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The notebook [https://jovian.ai/kara-mounir/mnist-autoencoder]  \r\n",
    "\r\n",
    "Machine learning mastery blog by Jason Brownlee [https://machinelearningmastery.com]  \r\n",
    "\r\n",
    "My Github [https://github.com/zaitrik]  \r\n",
    "\r\n",
    "My LinkedIn [https://www.linkedin.com/in/mounir-kara-zaitri-a01a00208/]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "684b1123683431d89d3bfe9a89cc763215f4b8cd94b4aba1fb40ad45ff7c8b41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}